{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T20:22:06.778053Z",
     "start_time": "2025-06-23T20:21:56.840567Z"
    }
   },
   "source": [
    "# Cell 1: Load the data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/knowledge_base.csv\")\n",
    "\n",
    "# Drop any rows that might have an empty knowledge_doc, just in case\n",
    "df.dropna(subset=['knowledge_doc'], inplace=True)\n",
    "\n",
    "# Take a smaller sample to work with for this comparison exercise\n",
    "sample_df = df.sample(1000, random_state=42)\n",
    "\n",
    "print(f\"Loaded {len(sample_df)} sample documents.\")\n",
    "sample_df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 sample documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        parent_asin                                      knowledge_doc\n",
       "534528   B000V4DOY4  Product Title: iPhone 8 Lightning Adapter Head...\n",
       "117607   B007WPHXA6  Product Title: Gear4 Angry Birds SpaceTouch Ca...\n",
       "1490858  B0BN8DDG2Z  Product Title: Surface Pro 9 Pro 8 Docking Sta...\n",
       "1411203  B09W5VRNDZ  Product Title: DriSentri 120W Speaker 3 Way Au...\n",
       "1602348  B07VHL6G2H  Product Title: USB Bluetooth 4.0 Adapter Dongl...\n",
       "...             ...                                                ...\n",
       "266385   B08NF1JGNJ  Product Title: USB C to USB C Cable, Anwaut 6....\n",
       "179185   B01MT5YSBW  Product Title: Acer Aspire Desktop, 7th Gen In...\n",
       "1271755  B08TZSRKB5  Product Title: iPhone Cable，MFi Certified Ligh...\n",
       "400631   B07CNG9TBM  Product Title: Bliiq Bluetooth Headphones, HiF...\n",
       "570314   B0838F3S3P  Product Title: Network RJ45 Splitter Adapter, ...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>knowledge_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>534528</th>\n",
       "      <td>B000V4DOY4</td>\n",
       "      <td>Product Title: iPhone 8 Lightning Adapter Head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117607</th>\n",
       "      <td>B007WPHXA6</td>\n",
       "      <td>Product Title: Gear4 Angry Birds SpaceTouch Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490858</th>\n",
       "      <td>B0BN8DDG2Z</td>\n",
       "      <td>Product Title: Surface Pro 9 Pro 8 Docking Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411203</th>\n",
       "      <td>B09W5VRNDZ</td>\n",
       "      <td>Product Title: DriSentri 120W Speaker 3 Way Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602348</th>\n",
       "      <td>B07VHL6G2H</td>\n",
       "      <td>Product Title: USB Bluetooth 4.0 Adapter Dongl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266385</th>\n",
       "      <td>B08NF1JGNJ</td>\n",
       "      <td>Product Title: USB C to USB C Cable, Anwaut 6....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179185</th>\n",
       "      <td>B01MT5YSBW</td>\n",
       "      <td>Product Title: Acer Aspire Desktop, 7th Gen In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271755</th>\n",
       "      <td>B08TZSRKB5</td>\n",
       "      <td>Product Title: iPhone Cable，MFi Certified Ligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400631</th>\n",
       "      <td>B07CNG9TBM</td>\n",
       "      <td>Product Title: Bliiq Bluetooth Headphones, HiF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570314</th>\n",
       "      <td>B0838F3S3P</td>\n",
       "      <td>Product Title: Network RJ45 Splitter Adapter, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T20:42:17.566219Z",
     "start_time": "2025-06-23T20:42:16.696197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the length of each document\n",
    "df['doc_length'] = df['knowledge_doc'].str.len()\n",
    "\n",
    "# Get the 5 longest documents\n",
    "longest_docs_df = df.nlargest(50, 'doc_length')\n",
    "\n",
    "longest_docs_df"
   ],
   "id": "bc491c36ea225755",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        parent_asin                                      knowledge_doc  \\\n",
       "49516    B002LSI1LO  Product Title: EOS 7D 18MP Digital SLR Camera\\...   \n",
       "528193   B006ZTVHD4  Product Title: Canon EOS C300 Cinema EOS Camco...   \n",
       "552947   B00UB8QO42  Product Title: Canon PowerShot G7 X Digital Ca...   \n",
       "2205     B00852VU86  Product Title: CLS110RG-W7AE\\nBrand: JL AUDIO\\...   \n",
       "1416569  B002TH2XO8  Product Title: 13W3v3-4\\nBrand: JL AUDIO\\nDesc...   \n",
       "714908   B001UDM4GE  Product Title: 13W3v3-2\\nBrand: JL AUDIO\\nDesc...   \n",
       "248478   B0000511YT  Product Title: Tripp Lite Hospital-Grade Isoba...   \n",
       "203208   B00MH5GYI8  Product Title: JL Audio ACS112LG-TW1 12\" 12TW1...   \n",
       "1311756  B00IF9M4KM  Product Title: JL Audio HO110-W6v3 Ported H.O....   \n",
       "1565405  B00B87NEFC  Product Title: Tripp Lite CPU Wall/Desk Mount ...   \n",
       "1254451  B001UDM4VY  Product Title: JL Audio H.O. Wedge Mobile Box ...   \n",
       "133636   B08PCCXXJ2  Product Title: Pixma Canon TS3322 Wireless All...   \n",
       "453929   B000WS0CRQ  Product Title: Tripp Lite 550VA Audio/Video Ba...   \n",
       "424853   B08P4JK77Y  Product Title: GIGABYTE [2020] AERO 15 OLED KB...   \n",
       "1532719  B01M9IU8E2  Product Title: Dominion® d110-GLOSS\\nBrand: JL...   \n",
       "897165   B0851JTSJS  Product Title: Gigabyte [2020] AERO 17 YB Thin...   \n",
       "460826   B08D2GQ18X  Product Title: GIGABYTE [2020] AERO 15 WB Thin...   \n",
       "1401027  B07W16Y5MF  Product Title: Gigabyte AERO 17 SA Thin+Light ...   \n",
       "891584   B00D06JHIM  Product Title: Tripp Lite TLP1008TELTV Datacom...   \n",
       "655048   B0C6XHTL4Q  Product Title: LG gram 17” Lightweight Laptop,...   \n",
       "177503   B07HXKR479  Product Title: C. Crane CC Skywave SSB AM, FM,...   \n",
       "629400   B005NJCWL8  Product Title: 15W0v3-4\\nBrand: JL AUDIO\\nFeat...   \n",
       "758681   B01MQ1YEYH  Product Title: EVGA PowerLink, Support All NVI...   \n",
       "163609   B07FT8ZBMR  Product Title: LG 34WK95U-W 34\"UltraWide 5K Na...   \n",
       "616619   B0778SF9Q5  Product Title: Amcrest UltraHD 2K WiFi Video S...   \n",
       "1441587  B0953ZL1M3  Product Title: LG AU810PB 4K UHD Smart Dual La...   \n",
       "581600   B0B64GTWPP  Product Title: LG gram 16” 2in1 Lightweight La...   \n",
       "449354   B09QRVR4WB  Product Title: HOUSE OF PARTY 13 Pcs Acrylic B...   \n",
       "1419754  B0B582BY8G  Product Title: Cable Clips Adhesive Cord Holde...   \n",
       "1062058  B00OS9NWJU  Product Title: XD500/3v2: 3 Ch. Class D System...   \n",
       "959324   B06Y54TK2B  Product Title: XFX AMD Radeon RX 580 8GB GDDR5...   \n",
       "902361   B08BCRJYRC  Product Title: 38” UltraGear Curved WQHD+ Nano...   \n",
       "467139   B001FSA8PS  Product Title: Mount World Universal Tilt Wall...   \n",
       "1542715  B0036S3VIQ  Product Title: Mount World 1133 Adjustable Til...   \n",
       "116118   B008XICI9M  Product Title: JL Audio Slash 300/4v3\\nBrand: ...   \n",
       "172895   B08QCTJ85S  Product Title: KFD 18V 19V 20V 65W Universal L...   \n",
       "1236239  B0BMS7NRDH  Product Title: Ricoh GR IIIx, Black, Digital C...   \n",
       "347175   B09LXKQMZY  Product Title: 32'' UltraGear QHD 165Hz HDR10 ...   \n",
       "1169711  B09FYMG5NR  Product Title: LG 55\" UHD 70 Series 4K HDR Sma...   \n",
       "29855    B09F14HJ1N  Product Title: LG UP7070 75-in 4K UHD 4K UHD 6...   \n",
       "1466943  B005D74M68  Product Title: C3-570\\nBrand: JL AUDIO\\nDescri...   \n",
       "908009   B096T7CG6N  Product Title: Amcrest 5MP UltraHD Mini AI Out...   \n",
       "809098   B0928X4253  Product Title: Ricoh WG-70 Black Waterproof Di...   \n",
       "908855   B00HZ4UXVK  Product Title: C. Crane Versa USB WiFi Adapter...   \n",
       "827410   B0037IHDIO  Product Title: Mount World 1019 Dual Arm Artic...   \n",
       "1550765  B005D74M5O  Product Title: C3-525\\nBrand: JL AUDIO\\nFeatur...   \n",
       "1007020  B004CT47VS  Product Title: JL Audio 0699440991025 Loudspea...   \n",
       "793080   B07MFRGMNX  Product Title: Amcrest UltraHD 2K WiFi Camera ...   \n",
       "744964   B07V821D2B  Product Title: Dayton Audio DATS V3 Computer B...   \n",
       "484195   B0BG4PPK34  Product Title: Sangean CL-100 NOAA, S.A.M.E an...   \n",
       "\n",
       "         doc_length  \n",
       "49516         51123  \n",
       "528193        50401  \n",
       "552947        32423  \n",
       "2205          21114  \n",
       "1416569       18294  \n",
       "714908        18290  \n",
       "248478        16753  \n",
       "203208        15784  \n",
       "1311756       15764  \n",
       "1565405       15523  \n",
       "1254451       14512  \n",
       "133636        13683  \n",
       "453929        13455  \n",
       "424853        13356  \n",
       "1532719       13294  \n",
       "897165        13064  \n",
       "460826        13034  \n",
       "1401027       13034  \n",
       "891584        12598  \n",
       "655048        12257  \n",
       "177503        12047  \n",
       "629400        11866  \n",
       "758681        11684  \n",
       "163609        11626  \n",
       "616619        11601  \n",
       "1441587       11583  \n",
       "581600        11525  \n",
       "449354        11355  \n",
       "1419754       11223  \n",
       "1062058       11155  \n",
       "959324        11001  \n",
       "902361        10899  \n",
       "467139        10724  \n",
       "1542715       10662  \n",
       "116118        10652  \n",
       "172895        10559  \n",
       "1236239       10512  \n",
       "347175        10440  \n",
       "1169711       10424  \n",
       "29855         10165  \n",
       "1466943       10000  \n",
       "908009         9957  \n",
       "809098         9805  \n",
       "908855         9731  \n",
       "827410         9703  \n",
       "1550765        9700  \n",
       "1007020        9699  \n",
       "793080         9526  \n",
       "744964         9474  \n",
       "484195         9416  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>knowledge_doc</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49516</th>\n",
       "      <td>B002LSI1LO</td>\n",
       "      <td>Product Title: EOS 7D 18MP Digital SLR Camera\\...</td>\n",
       "      <td>51123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528193</th>\n",
       "      <td>B006ZTVHD4</td>\n",
       "      <td>Product Title: Canon EOS C300 Cinema EOS Camco...</td>\n",
       "      <td>50401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552947</th>\n",
       "      <td>B00UB8QO42</td>\n",
       "      <td>Product Title: Canon PowerShot G7 X Digital Ca...</td>\n",
       "      <td>32423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>B00852VU86</td>\n",
       "      <td>Product Title: CLS110RG-W7AE\\nBrand: JL AUDIO\\...</td>\n",
       "      <td>21114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416569</th>\n",
       "      <td>B002TH2XO8</td>\n",
       "      <td>Product Title: 13W3v3-4\\nBrand: JL AUDIO\\nDesc...</td>\n",
       "      <td>18294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714908</th>\n",
       "      <td>B001UDM4GE</td>\n",
       "      <td>Product Title: 13W3v3-2\\nBrand: JL AUDIO\\nDesc...</td>\n",
       "      <td>18290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248478</th>\n",
       "      <td>B0000511YT</td>\n",
       "      <td>Product Title: Tripp Lite Hospital-Grade Isoba...</td>\n",
       "      <td>16753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203208</th>\n",
       "      <td>B00MH5GYI8</td>\n",
       "      <td>Product Title: JL Audio ACS112LG-TW1 12\" 12TW1...</td>\n",
       "      <td>15784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311756</th>\n",
       "      <td>B00IF9M4KM</td>\n",
       "      <td>Product Title: JL Audio HO110-W6v3 Ported H.O....</td>\n",
       "      <td>15764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565405</th>\n",
       "      <td>B00B87NEFC</td>\n",
       "      <td>Product Title: Tripp Lite CPU Wall/Desk Mount ...</td>\n",
       "      <td>15523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254451</th>\n",
       "      <td>B001UDM4VY</td>\n",
       "      <td>Product Title: JL Audio H.O. Wedge Mobile Box ...</td>\n",
       "      <td>14512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133636</th>\n",
       "      <td>B08PCCXXJ2</td>\n",
       "      <td>Product Title: Pixma Canon TS3322 Wireless All...</td>\n",
       "      <td>13683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453929</th>\n",
       "      <td>B000WS0CRQ</td>\n",
       "      <td>Product Title: Tripp Lite 550VA Audio/Video Ba...</td>\n",
       "      <td>13455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424853</th>\n",
       "      <td>B08P4JK77Y</td>\n",
       "      <td>Product Title: GIGABYTE [2020] AERO 15 OLED KB...</td>\n",
       "      <td>13356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532719</th>\n",
       "      <td>B01M9IU8E2</td>\n",
       "      <td>Product Title: Dominion® d110-GLOSS\\nBrand: JL...</td>\n",
       "      <td>13294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897165</th>\n",
       "      <td>B0851JTSJS</td>\n",
       "      <td>Product Title: Gigabyte [2020] AERO 17 YB Thin...</td>\n",
       "      <td>13064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460826</th>\n",
       "      <td>B08D2GQ18X</td>\n",
       "      <td>Product Title: GIGABYTE [2020] AERO 15 WB Thin...</td>\n",
       "      <td>13034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401027</th>\n",
       "      <td>B07W16Y5MF</td>\n",
       "      <td>Product Title: Gigabyte AERO 17 SA Thin+Light ...</td>\n",
       "      <td>13034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891584</th>\n",
       "      <td>B00D06JHIM</td>\n",
       "      <td>Product Title: Tripp Lite TLP1008TELTV Datacom...</td>\n",
       "      <td>12598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655048</th>\n",
       "      <td>B0C6XHTL4Q</td>\n",
       "      <td>Product Title: LG gram 17” Lightweight Laptop,...</td>\n",
       "      <td>12257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177503</th>\n",
       "      <td>B07HXKR479</td>\n",
       "      <td>Product Title: C. Crane CC Skywave SSB AM, FM,...</td>\n",
       "      <td>12047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629400</th>\n",
       "      <td>B005NJCWL8</td>\n",
       "      <td>Product Title: 15W0v3-4\\nBrand: JL AUDIO\\nFeat...</td>\n",
       "      <td>11866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758681</th>\n",
       "      <td>B01MQ1YEYH</td>\n",
       "      <td>Product Title: EVGA PowerLink, Support All NVI...</td>\n",
       "      <td>11684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163609</th>\n",
       "      <td>B07FT8ZBMR</td>\n",
       "      <td>Product Title: LG 34WK95U-W 34\"UltraWide 5K Na...</td>\n",
       "      <td>11626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616619</th>\n",
       "      <td>B0778SF9Q5</td>\n",
       "      <td>Product Title: Amcrest UltraHD 2K WiFi Video S...</td>\n",
       "      <td>11601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441587</th>\n",
       "      <td>B0953ZL1M3</td>\n",
       "      <td>Product Title: LG AU810PB 4K UHD Smart Dual La...</td>\n",
       "      <td>11583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581600</th>\n",
       "      <td>B0B64GTWPP</td>\n",
       "      <td>Product Title: LG gram 16” 2in1 Lightweight La...</td>\n",
       "      <td>11525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449354</th>\n",
       "      <td>B09QRVR4WB</td>\n",
       "      <td>Product Title: HOUSE OF PARTY 13 Pcs Acrylic B...</td>\n",
       "      <td>11355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419754</th>\n",
       "      <td>B0B582BY8G</td>\n",
       "      <td>Product Title: Cable Clips Adhesive Cord Holde...</td>\n",
       "      <td>11223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062058</th>\n",
       "      <td>B00OS9NWJU</td>\n",
       "      <td>Product Title: XD500/3v2: 3 Ch. Class D System...</td>\n",
       "      <td>11155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959324</th>\n",
       "      <td>B06Y54TK2B</td>\n",
       "      <td>Product Title: XFX AMD Radeon RX 580 8GB GDDR5...</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902361</th>\n",
       "      <td>B08BCRJYRC</td>\n",
       "      <td>Product Title: 38” UltraGear Curved WQHD+ Nano...</td>\n",
       "      <td>10899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467139</th>\n",
       "      <td>B001FSA8PS</td>\n",
       "      <td>Product Title: Mount World Universal Tilt Wall...</td>\n",
       "      <td>10724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542715</th>\n",
       "      <td>B0036S3VIQ</td>\n",
       "      <td>Product Title: Mount World 1133 Adjustable Til...</td>\n",
       "      <td>10662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116118</th>\n",
       "      <td>B008XICI9M</td>\n",
       "      <td>Product Title: JL Audio Slash 300/4v3\\nBrand: ...</td>\n",
       "      <td>10652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172895</th>\n",
       "      <td>B08QCTJ85S</td>\n",
       "      <td>Product Title: KFD 18V 19V 20V 65W Universal L...</td>\n",
       "      <td>10559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236239</th>\n",
       "      <td>B0BMS7NRDH</td>\n",
       "      <td>Product Title: Ricoh GR IIIx, Black, Digital C...</td>\n",
       "      <td>10512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347175</th>\n",
       "      <td>B09LXKQMZY</td>\n",
       "      <td>Product Title: 32'' UltraGear QHD 165Hz HDR10 ...</td>\n",
       "      <td>10440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169711</th>\n",
       "      <td>B09FYMG5NR</td>\n",
       "      <td>Product Title: LG 55\" UHD 70 Series 4K HDR Sma...</td>\n",
       "      <td>10424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29855</th>\n",
       "      <td>B09F14HJ1N</td>\n",
       "      <td>Product Title: LG UP7070 75-in 4K UHD 4K UHD 6...</td>\n",
       "      <td>10165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466943</th>\n",
       "      <td>B005D74M68</td>\n",
       "      <td>Product Title: C3-570\\nBrand: JL AUDIO\\nDescri...</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908009</th>\n",
       "      <td>B096T7CG6N</td>\n",
       "      <td>Product Title: Amcrest 5MP UltraHD Mini AI Out...</td>\n",
       "      <td>9957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809098</th>\n",
       "      <td>B0928X4253</td>\n",
       "      <td>Product Title: Ricoh WG-70 Black Waterproof Di...</td>\n",
       "      <td>9805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908855</th>\n",
       "      <td>B00HZ4UXVK</td>\n",
       "      <td>Product Title: C. Crane Versa USB WiFi Adapter...</td>\n",
       "      <td>9731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827410</th>\n",
       "      <td>B0037IHDIO</td>\n",
       "      <td>Product Title: Mount World 1019 Dual Arm Artic...</td>\n",
       "      <td>9703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550765</th>\n",
       "      <td>B005D74M5O</td>\n",
       "      <td>Product Title: C3-525\\nBrand: JL AUDIO\\nFeatur...</td>\n",
       "      <td>9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007020</th>\n",
       "      <td>B004CT47VS</td>\n",
       "      <td>Product Title: JL Audio 0699440991025 Loudspea...</td>\n",
       "      <td>9699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793080</th>\n",
       "      <td>B07MFRGMNX</td>\n",
       "      <td>Product Title: Amcrest UltraHD 2K WiFi Camera ...</td>\n",
       "      <td>9526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744964</th>\n",
       "      <td>B07V821D2B</td>\n",
       "      <td>Product Title: Dayton Audio DATS V3 Computer B...</td>\n",
       "      <td>9474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484195</th>\n",
       "      <td>B0BG4PPK34</td>\n",
       "      <td>Product Title: Sangean CL-100 NOAA, S.A.M.E an...</td>\n",
       "      <td>9416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:18:40.571910Z",
     "start_time": "2025-06-23T21:18:40.562700Z"
    }
   },
   "cell_type": "code",
   "source": "len(df)",
   "id": "6263dafeeaf0864",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:18:18.005764Z",
     "start_time": "2025-06-23T21:18:17.667104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This assumes 'df' is a pandas DataFrame that has been loaded\n",
    "# with an 'asin' column.\n",
    "\n",
    "# The ASIN to search for\n",
    "target_asin = 'B08BHHSB6M'\n",
    "\n",
    "# Find the row(s) with the matching ASIN\n",
    "# The 'asin' column might have leading/trailing spaces, so we use .str.strip()\n",
    "result_row = df[df['parent_asin'].str.strip() == target_asin]\n",
    "\n",
    "# Print the result\n",
    "if not result_row.empty:\n",
    "    print(f\"Found row with ASIN: {target_asin}\\n\")\n",
    "    # Print the full content of the row\n",
    "    for index, row in result_row.iterrows():\n",
    "        print(row.to_string())\n",
    "else:\n",
    "    print(f\"No row found with ASIN: {target_asin}\")"
   ],
   "id": "56f291647315da85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No row found with ASIN: B08BHHSB6M\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T20:59:49.729569Z",
     "start_time": "2025-06-22T20:59:41.951859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: Generate embeddings with local model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained, all-around model. This will be downloaded once.\n",
    "local_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get the list of documents to embed\n",
    "documents = sample_df['knowledge_doc'].tolist()\n",
    "\n",
    "print(\"Generating embeddings with the local model...\")\n",
    "local_embeddings = local_model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings generated. Shape: {local_embeddings.shape}\")\n",
    "# The output shape will be (1000, 384), meaning 1000 vectors, each with 384 dimensions."
   ],
   "id": "7d08923c20d75acc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with the local model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d8627a5692046d1bc4a0ecd311d7a0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated. Shape: (1000, 384)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T21:00:01.169006Z",
     "start_time": "2025-06-22T20:59:49.805225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3: Generate embeddings with Gemini API\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "print(\"Generating embeddings with the Gemini API...\")\n",
    "# The 'text-embedding-004' model is Google's latest embedding model\n",
    "# Note: The API has rate limits, so we send documents in batches.\n",
    "result = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    content=documents,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\" # Important: Specify the task type\n",
    ")\n",
    "\n",
    "gemini_embeddings = result['embedding']\n",
    "\n",
    "print(f\"Embeddings generated. Shape: {np.array(gemini_embeddings).shape}\")\n",
    "# The output shape will be (1000, 768), meaning 1000 vectors, each with 768 dimensions."
   ],
   "id": "aa6abe7c031d8464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with the Gemini API...\n",
      "Embeddings generated. Shape: (1000, 768)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T21:48:02.687402Z",
     "start_time": "2025-06-22T21:43:32.184019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.asyncio import tqdm\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# ======================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ======================================================================================\n",
    "# Make sure your .env file is in the project root, and you run this notebook from there.\n",
    "load_dotenv()\n",
    "\n",
    "# --- API Keys ---\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# --- Pinecone Config ---\n",
    "INDEX_NAME = \"electronics-gemini\"\n",
    "CLOUD_PROVIDER = \"aws\"\n",
    "CLOUD_REGION = \"us-east-1\"\n",
    "# Pinecone's official limit is 2MB per request. We'll use a slightly smaller\n",
    "# value to be safe and account for overhead.\n",
    "MAX_PINECONE_REQUEST_SIZE_BYTES = 2 * 1024 * 1024 * 0.9\n",
    "\n",
    "# --- Gemini Config ---\n",
    "# The Gemini API also has size limits. We'll create text batches that are\n",
    "# roughly under a safe size limit to avoid errors. 1MB is a safe starting point.\n",
    "MAX_GEMINI_REQUEST_SIZE_BYTES = 1 * 1024 * 1024\n",
    "GEMINI_RPM_LIMIT = 60\n",
    "DELAY_BETWEEN_REQUESTS = 60.0 / GEMINI_RPM_LIMIT\n",
    "\n",
    "# --- Data Config ---\n",
    "DATA_FILE_PATH = \"../data/knowledge_base.csv\"\n",
    "NUM_DOCUMENTS_TO_PROCESS = 20000\n",
    "\n",
    "# ======================================================================================\n",
    "# 2. HELPER FUNCTIONS FOR ADAPTIVE BATCHING\n",
    "# ======================================================================================\n",
    "\n",
    "def create_adaptive_text_batches(documents_with_ids):\n",
    "    \"\"\"\n",
    "    A generator that yields size-optimized batches of text documents for the Gemini API.\n",
    "    \"\"\"\n",
    "    current_batch_docs = []\n",
    "    current_batch_ids = []\n",
    "    current_batch_size = 0\n",
    "\n",
    "    for doc, doc_id in documents_with_ids:\n",
    "        doc_size = sys.getsizeof(doc)\n",
    "\n",
    "        if current_batch_size + doc_size > MAX_GEMINI_REQUEST_SIZE_BYTES and current_batch_docs:\n",
    "            yield current_batch_docs, current_batch_ids\n",
    "            current_batch_docs = []\n",
    "            current_batch_ids = []\n",
    "            current_batch_size = 0\n",
    "\n",
    "        current_batch_docs.append(doc)\n",
    "        current_batch_ids.append(doc_id)\n",
    "        current_batch_size += doc_size\n",
    "\n",
    "    if current_batch_docs:\n",
    "        yield current_batch_docs, current_batch_ids\n",
    "\n",
    "def create_adaptive_pinecone_batches(vectors_with_ids):\n",
    "    \"\"\"\n",
    "    A generator that yields size-optimized batches for Pinecone upsert.\n",
    "    \"\"\"\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "\n",
    "    for vec_with_id in vectors_with_ids:\n",
    "        vector_size = sys.getsizeof(vec_with_id[1]) + sys.getsizeof(vec_with_id[0])\n",
    "\n",
    "        if current_batch_size + vector_size > MAX_PINECONE_REQUEST_SIZE_BYTES and current_batch:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "            current_batch_size = 0\n",
    "\n",
    "        current_batch.append(vec_with_id)\n",
    "        current_batch_size += vector_size\n",
    "\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "# ======================================================================================\n",
    "# 3. ASYNC PIPELINE COMPONENTS\n",
    "# ======================================================================================\n",
    "\n",
    "async def producer_generate_embeddings(queue: asyncio.Queue, documents_with_ids: list):\n",
    "    \"\"\"\n",
    "    An async \"producer\" that generates embeddings for adaptively sized batches\n",
    "    of documents and puts them into a queue for the consumer.\n",
    "    \"\"\"\n",
    "    print(\"Starting embedding generation with adaptive batches...\")\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "    batch_generator = create_adaptive_text_batches(documents_with_ids)\n",
    "\n",
    "    for batch_docs, batch_ids in tqdm(batch_generator, desc=\"Generating Embeddings\"):\n",
    "        try:\n",
    "            result = await genai.embed_content_async(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                content=batch_docs,\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "            )\n",
    "            # Pair the generated embeddings with their original IDs\n",
    "            embeddings_with_ids = list(zip(batch_ids, result['embedding']))\n",
    "            await queue.put(embeddings_with_ids)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during embedding generation: {e}\")\n",
    "\n",
    "        await asyncio.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "    await queue.put(None)\n",
    "    print(\"Embedding generation finished.\")\n",
    "\n",
    "\n",
    "async def consumer_upsert_to_pinecone(queue: asyncio.Queue, index: Pinecone.Index):\n",
    "    \"\"\"\n",
    "    An async \"consumer\" that gets generated embeddings, creates adaptive\n",
    "    sub-batches, and upserts them to Pinecone.\n",
    "    \"\"\"\n",
    "    print(\"Starting Pinecone ingestion with adaptive batches...\")\n",
    "    while True:\n",
    "        embedding_batch = await queue.get()\n",
    "\n",
    "        if embedding_batch is None:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Create size-optimized sub-batches from the received embedding batch\n",
    "            for adaptive_batch in create_adaptive_pinecone_batches(embedding_batch):\n",
    "                loop = asyncio.get_running_loop()\n",
    "                await loop.run_in_executor(None, lambda: index.upsert(vectors=adaptive_batch))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during Pinecone upsert: {e}\")\n",
    "\n",
    "        queue.task_done()\n",
    "\n",
    "    print(\"Pinecone ingestion finished.\")\n",
    "\n",
    "# ======================================================================================\n",
    "# 4. MAIN ORCHESTRATOR\n",
    "# ======================================================================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function to orchestrate the entire pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Loading data from {DATA_FILE_PATH}...\")\n",
    "    df = pd.read_csv(DATA_FILE_PATH).head(NUM_DOCUMENTS_TO_PROCESS)\n",
    "    df.dropna(subset=['knowledge_doc'], inplace=True)\n",
    "    # Create a list of (document, id) tuples to pass to the producer\n",
    "    documents_with_ids = list(zip(df['knowledge_doc'], df['parent_asin'].astype(str)))\n",
    "    print(f\"Loaded {len(documents_with_ids)} documents to process.\")\n",
    "\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    if INDEX_NAME not in pc.list_indexes().names():\n",
    "        print(f\"Creating a new serverless index: {INDEX_NAME}\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=768, metric='cosine',\n",
    "            spec=ServerlessSpec(cloud=CLOUD_PROVIDER, region=CLOUD_REGION)\n",
    "        )\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    print(\"Pinecone initialized.\")\n",
    "\n",
    "    embedding_queue = asyncio.Queue(maxsize=10)\n",
    "\n",
    "    producer_task = asyncio.create_task(producer_generate_embeddings(embedding_queue, documents_with_ids))\n",
    "    consumer_task = asyncio.create_task(consumer_upsert_to_pinecone(embedding_queue, index))\n",
    "\n",
    "    await asyncio.gather(producer_task, consumer_task)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nPipeline finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"Final index stats:\")\n",
    "    print(index.describe_index_stats())\n",
    "\n",
    "# ======================================================================================\n",
    "# 5. EXECUTE THE PIPELINE\n",
    "# ======================================================================================\n",
    "await main()\n"
   ],
   "id": "6b9b1892eeb301a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/knowledge_base.csv...\n",
      "Loaded 20000 documents to process.\n",
      "Initializing Pinecone...\n",
      "Pinecone initialized.\n",
      "Starting embedding generation with adaptive batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pinecone ingestion with adaptive batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 29it [04:16,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation finished.\n",
      "Pinecone ingestion finished.\n",
      "\n",
      "Pipeline finished in 270.33 seconds.\n",
      "Final index stats:\n",
      "{'dimension': 768,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 40000}},\n",
      " 'total_vector_count': 40000,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T21:51:56.776255Z",
     "start_time": "2025-06-22T21:50:35.921925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.asyncio import tqdm\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# ======================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ======================================================================================\n",
    "# Make sure your .env file is in the project root, and you run this notebook from there.\n",
    "load_dotenv()\n",
    "\n",
    "# --- API Keys ---\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# --- Pinecone Config ---\n",
    "INDEX_NAME = \"electronics-gemini\"\n",
    "CLOUD_PROVIDER = \"aws\"\n",
    "CLOUD_REGION = \"us-east-1\"\n",
    "MAX_PINECONE_REQUEST_SIZE_BYTES = 2 * 1024 * 1024 * 0.9\n",
    "\n",
    "# --- Gemini Config ---\n",
    "MAX_GEMINI_REQUEST_SIZE_BYTES = 1 * 1024 * 1024\n",
    "GEMINI_RPM_LIMIT = 60\n",
    "DELAY_BETWEEN_REQUESTS = 60.0 / GEMINI_RPM_LIMIT\n",
    "\n",
    "# --- Data Config ---\n",
    "DATA_FILE_PATH = \"../data/knowledge_base.csv\"\n",
    "NUM_DOCUMENTS_TO_PROCESS = 20000\n",
    "\n",
    "# --- Concurrency Config ---\n",
    "# We will create multiple \"workers\" to make API calls concurrently\n",
    "NUM_PRODUCER_WORKERS = 5  # Number of parallel tasks calling the Gemini API\n",
    "NUM_CONSUMER_WORKERS = 5  # Number of parallel tasks upserting to Pinecone\n",
    "\n",
    "# ======================================================================================\n",
    "# 2. HELPER FUNCTIONS FOR ADAPTIVE BATCHING\n",
    "# ======================================================================================\n",
    "\n",
    "def create_adaptive_text_batches(documents_with_ids):\n",
    "    \"\"\"\n",
    "    A generator that yields size-optimized batches of text documents for the Gemini API.\n",
    "    \"\"\"\n",
    "    current_batch_docs = []\n",
    "    current_batch_ids = []\n",
    "    current_batch_size = 0\n",
    "\n",
    "    for doc, doc_id in documents_with_ids:\n",
    "        doc_size = sys.getsizeof(doc)\n",
    "\n",
    "        if current_batch_size + doc_size > MAX_GEMINI_REQUEST_SIZE_BYTES and current_batch_docs:\n",
    "            yield current_batch_docs, current_batch_ids\n",
    "            current_batch_docs = []\n",
    "            current_batch_ids = []\n",
    "            current_batch_size = 0\n",
    "\n",
    "        current_batch_docs.append(doc)\n",
    "        current_batch_ids.append(doc_id)\n",
    "        current_batch_size += doc_size\n",
    "\n",
    "    if current_batch_docs:\n",
    "        yield current_batch_docs, current_batch_ids\n",
    "\n",
    "def create_adaptive_pinecone_batches(vectors_with_ids):\n",
    "    \"\"\"\n",
    "    A generator that yields size-optimized batches for Pinecone upsert.\n",
    "    \"\"\"\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "\n",
    "    for vec_with_id in vectors_with_ids:\n",
    "        vector_size = sys.getsizeof(vec_with_id[1]) + sys.getsizeof(vec_with_id[0])\n",
    "\n",
    "        if current_batch_size + vector_size > MAX_PINECONE_REQUEST_SIZE_BYTES and current_batch:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "            current_batch_size = 0\n",
    "\n",
    "        current_batch.append(vec_with_id)\n",
    "        current_batch_size += vector_size\n",
    "\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "# ======================================================================================\n",
    "# 3. ASYNC PIPELINE COMPONENTS\n",
    "# ======================================================================================\n",
    "\n",
    "async def producer_worker(worker_id: int, queue: asyncio.Queue, documents_with_ids: list):\n",
    "    \"\"\"\n",
    "    A single producer worker that generates embeddings for its assigned share of documents.\n",
    "    \"\"\"\n",
    "    print(f\"[Producer-{worker_id}] Starting...\")\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "    batch_generator = create_adaptive_text_batches(documents_with_ids)\n",
    "\n",
    "    for batch_docs, batch_ids in batch_generator:\n",
    "        try:\n",
    "            result = await genai.embed_content_async(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                content=batch_docs,\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "            )\n",
    "            embeddings_with_ids = list(zip(batch_ids, result['embedding']))\n",
    "            await queue.put(embeddings_with_ids)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Producer-{worker_id}] Error: {e}\")\n",
    "\n",
    "        await asyncio.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "    print(f\"[Producer-{worker_id}] Finished.\")\n",
    "\n",
    "\n",
    "async def consumer_worker(worker_id: int, queue: asyncio.Queue, index: Pinecone.Index):\n",
    "    \"\"\"\n",
    "    A single consumer worker that gets embeddings from the queue and upserts them.\n",
    "    \"\"\"\n",
    "    print(f\"[Consumer-{worker_id}] Starting...\")\n",
    "    while True:\n",
    "        embedding_batch = await queue.get()\n",
    "\n",
    "        if embedding_batch is None:\n",
    "            # Re-add None to the queue for other consumers to see\n",
    "            await queue.put(None)\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            for adaptive_batch in create_adaptive_pinecone_batches(embedding_batch):\n",
    "                loop = asyncio.get_running_loop()\n",
    "                await loop.run_in_executor(None, lambda: index.upsert(vectors=adaptive_batch))\n",
    "        except Exception as e:\n",
    "            print(f\"[Consumer-{worker_id}] Error: {e}\")\n",
    "\n",
    "        queue.task_done()\n",
    "\n",
    "    print(f\"[Consumer-{worker_id}] Finished.\")\n",
    "\n",
    "# ======================================================================================\n",
    "# 4. MAIN ORCHESTRATOR\n",
    "# ======================================================================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function to orchestrate the multi-worker pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Loading data from {DATA_FILE_PATH}...\")\n",
    "    df = pd.read_csv(DATA_FILE_PATH).head(NUM_DOCUMENTS_TO_PROCESS)\n",
    "    df.dropna(subset=['knowledge_doc'], inplace=True)\n",
    "    all_documents_with_ids = list(zip(df['knowledge_doc'], df['parent_asin'].astype(str)))\n",
    "    print(f\"Loaded {len(all_documents_with_ids)} documents to process.\")\n",
    "\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    if INDEX_NAME not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=768, metric='cosine',\n",
    "            spec=ServerlessSpec(cloud=CLOUD_PROVIDER, region=CLOUD_REGION)\n",
    "        )\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    print(\"Pinecone initialized.\")\n",
    "\n",
    "    # The shared queue for all workers\n",
    "    embedding_queue = asyncio.Queue(maxsize=20)\n",
    "\n",
    "    # Create and start consumer workers first\n",
    "    consumer_tasks = [\n",
    "        asyncio.create_task(consumer_worker(i, embedding_queue, index))\n",
    "        for i in range(NUM_CONSUMER_WORKERS)\n",
    "    ]\n",
    "\n",
    "    # Split the documents among the producer workers\n",
    "    docs_per_worker = len(all_documents_with_ids) // NUM_PRODUCER_WORKERS\n",
    "    producer_tasks = []\n",
    "    for i in range(NUM_PRODUCER_WORKERS):\n",
    "        start_index = i * docs_per_worker\n",
    "        # Give the last worker all remaining documents\n",
    "        end_index = (i + 1) * docs_per_worker if i < NUM_PRODUCER_WORKERS - 1 else len(all_documents_with_ids)\n",
    "        worker_docs = all_documents_with_ids[start_index:end_index]\n",
    "        task = asyncio.create_task(producer_worker(i, embedding_queue, worker_docs))\n",
    "        producer_tasks.append(task)\n",
    "\n",
    "    # Wait for all producers to finish\n",
    "    await asyncio.gather(*producer_tasks)\n",
    "    print(\"\\nAll producer workers have finished.\")\n",
    "\n",
    "    # Once producers are done, signal to the consumers to shut down\n",
    "    await embedding_queue.put(None)\n",
    "\n",
    "    # Wait for all consumers to finish\n",
    "    await asyncio.gather(*consumer_tasks)\n",
    "    print(\"All consumer workers have finished.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nPipeline finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"Final index stats:\")\n",
    "    print(index.describe_index_stats())\n",
    "\n",
    "# ======================================================================================\n",
    "# 5. EXECUTE THE PIPELINE\n",
    "# ======================================================================================\n",
    "await main()\n"
   ],
   "id": "25ab507fa2aa71c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/knowledge_base.csv...\n",
      "Loaded 20000 documents to process.\n",
      "Initializing Pinecone...\n",
      "Pinecone initialized.\n",
      "[Consumer-0] Starting...\n",
      "[Consumer-1] Starting...\n",
      "[Consumer-2] Starting...\n",
      "[Consumer-3] Starting...\n",
      "[Consumer-4] Starting...\n",
      "[Producer-0] Starting...\n",
      "[Producer-1] Starting...\n",
      "[Producer-2] Starting...\n",
      "[Producer-3] Starting...\n",
      "[Producer-4] Starting...\n",
      "[Producer-3] Finished.\n",
      "[Producer-4] Finished.\n",
      "[Producer-2] Finished.\n",
      "[Producer-0] Finished.\n",
      "[Producer-1] Finished.\n",
      "\n",
      "All producer workers have finished.\n",
      "[Consumer-0] Finished.\n",
      "[Consumer-3] Finished.\n",
      "[Consumer-1] Finished.\n",
      "[Consumer-4] Finished.\n",
      "[Consumer-2] Finished.\n",
      "All consumer workers have finished.\n",
      "\n",
      "Pipeline finished in 80.72 seconds.\n",
      "Final index stats:\n",
      "{'dimension': 768,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 40000}},\n",
      " 'total_vector_count': 40000,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T22:26:06.585697Z",
     "start_time": "2025-06-22T22:25:02.551613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.asyncio import tqdm\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# ======================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ======================================================================================\n",
    "# Make sure your .env file is in the project root.\n",
    "load_dotenv()\n",
    "\n",
    "# --- API Keys ---\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# --- Pinecone Config ---\n",
    "INDEX_NAME = \"electronics-gemini\"\n",
    "CLOUD_PROVIDER = \"aws\"\n",
    "CLOUD_REGION = \"us-east-1\"\n",
    "MAX_PINECONE_REQUEST_SIZE_BYTES = 2 * 1024 * 1024 * 0.9\n",
    "\n",
    "# --- Gemini Config ---\n",
    "MAX_GEMINI_REQUEST_SIZE_BYTES = 1 * 1024 * 1024\n",
    "# Set this to your account's RPM limit to calculate the delay.\n",
    "# Updated to 120 RPM as requested.\n",
    "GEMINI_RPM_LIMIT = 120\n",
    "\n",
    "# --- Data Config ---\n",
    "# Path is now relative to the notebook's location in the /notebooks directory\n",
    "DATA_FILE_PATH = \"../data/knowledge_base.csv\"\n",
    "NUM_DOCUMENTS_TO_PROCESS = 20000\n",
    "\n",
    "# --- Concurrency Config ---\n",
    "# We can keep a higher number of workers as the rate limiter will manage the API calls.\n",
    "NUM_PRODUCER_WORKERS = 10\n",
    "NUM_CONSUMER_WORKERS = 30\n",
    "\n",
    "# ======================================================================================\n",
    "# 2. ASYNC RATE LIMITER\n",
    "# ======================================================================================\n",
    "\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"\n",
    "    A robust asynchronous rate limiter that enforces a specified number of calls\n",
    "    per minute. It uses a token bucket-like approach to control the request rate\n",
    "    across all concurrent workers.\n",
    "    \"\"\"\n",
    "    def __init__(self, rate_limit: int, period_seconds: int = 60):\n",
    "        self.rate_limit = rate_limit\n",
    "        # Calculate the required delay between each request to not exceed the rate limit\n",
    "        self.delay = period_seconds / rate_limit\n",
    "        self._lock = asyncio.Lock()\n",
    "        self._last_request_time = 0\n",
    "\n",
    "    async def acquire(self):\n",
    "        \"\"\"Acquires a permit from the rate limiter, blocking if necessary.\"\"\"\n",
    "        async with self._lock:\n",
    "            current_time = time.monotonic()\n",
    "            time_since_last = current_time - self._last_request_time\n",
    "\n",
    "            # If the time since the last request is less than the required delay,\n",
    "            # wait for the remaining time.\n",
    "            if time_since_last < self.delay:\n",
    "                await asyncio.sleep(self.delay - time_since_last)\n",
    "\n",
    "            # Update the time of the last request to the current time\n",
    "            self._last_request_time = time.monotonic()\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# 3. HELPER FUNCTIONS FOR ADAPTIVE BATCHING\n",
    "# ======================================================================================\n",
    "\n",
    "def create_adaptive_text_batches(documents_with_ids):\n",
    "    # ... (This function remains unchanged)\n",
    "    current_batch_docs = []\n",
    "    current_batch_ids = []\n",
    "    current_batch_size = 0\n",
    "    for doc, doc_id in documents_with_ids:\n",
    "        doc_size = sys.getsizeof(doc)\n",
    "        if current_batch_size + doc_size > MAX_GEMINI_REQUEST_SIZE_BYTES and current_batch_docs:\n",
    "            yield current_batch_docs, current_batch_ids\n",
    "            current_batch_docs, current_batch_ids, current_batch_size = [], [], 0\n",
    "        current_batch_docs.append(doc)\n",
    "        current_batch_ids.append(doc_id)\n",
    "        current_batch_size += doc_size\n",
    "    if current_batch_docs:\n",
    "        yield current_batch_docs, current_batch_ids\n",
    "\n",
    "def create_adaptive_pinecone_batches(vectors_with_ids):\n",
    "    # ... (This function remains unchanged)\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "    for vec_with_id in vectors_with_ids:\n",
    "        vector_size = sys.getsizeof(vec_with_id[1]) + sys.getsizeof(vec_with_id[0])\n",
    "        if current_batch_size + vector_size > MAX_PINECONE_REQUEST_SIZE_BYTES and current_batch:\n",
    "            yield current_batch\n",
    "            current_batch, current_batch_size = [], 0\n",
    "        current_batch.append(vec_with_id)\n",
    "        current_batch_size += vector_size\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "# ======================================================================================\n",
    "# 4. ASYNC PIPELINE COMPONENTS\n",
    "# ======================================================================================\n",
    "\n",
    "async def producer_worker(worker_id: int, queue: asyncio.Queue, documents_with_ids: list, rate_limiter: AsyncRateLimiter):\n",
    "    \"\"\"\n",
    "    A single producer worker that uses the shared rate limiter before making API calls.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"[Producer-{worker_id}] Starting...\")\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "    batch_generator = create_adaptive_text_batches(documents_with_ids)\n",
    "\n",
    "    total_docs_processed = 0\n",
    "    for batch_docs, batch_ids in batch_generator:\n",
    "        try:\n",
    "            # *** THIS IS THE FIX: Acquire a permit from the rate limiter ***\n",
    "            await rate_limiter.acquire()\n",
    "\n",
    "            result = await genai.embed_content_async(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                content=batch_docs,\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "            )\n",
    "            embeddings_with_ids = list(zip(batch_ids, result['embedding']))\n",
    "            total_docs_processed += len(embeddings_with_ids)\n",
    "            await queue.put(embeddings_with_ids)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Producer-{worker_id}] Error: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[Producer-{worker_id}] Finished. Processed {total_docs_processed} docs in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "async def consumer_worker(worker_id: int, queue: asyncio.Queue, index: Pinecone.Index):\n",
    "    # ... (This function remains unchanged)\n",
    "    start_time = time.time()\n",
    "    print(f\"[Consumer-{worker_id}] Starting...\")\n",
    "    total_vectors_processed = 0\n",
    "    while True:\n",
    "        embedding_batch = await queue.get()\n",
    "        if embedding_batch is None:\n",
    "            await queue.put(None)\n",
    "            break\n",
    "        try:\n",
    "            for i, adaptive_batch in enumerate(create_adaptive_pinecone_batches(embedding_batch)):\n",
    "                batch_vector_count = len(adaptive_batch)\n",
    "                total_vectors_processed += batch_vector_count\n",
    "                loop = asyncio.get_running_loop()\n",
    "                await loop.run_in_executor(None, lambda: index.upsert(vectors=adaptive_batch))\n",
    "        except Exception as e:\n",
    "            print(f\"[Consumer-{worker_id}] Error: {e}\")\n",
    "        queue.task_done()\n",
    "    end_time = time.time()\n",
    "    print(f\"[Consumer-{worker_id}] Finished. Processed {total_vectors_processed} vectors in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ======================================================================================\n",
    "# 5. MAIN ORCHESTRATOR\n",
    "# ======================================================================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function to orchestrate the multi-worker pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Loading data from {DATA_FILE_PATH}...\")\n",
    "    df = pd.read_csv(DATA_FILE_PATH).head(NUM_DOCUMENTS_TO_PROCESS)\n",
    "    df.dropna(subset=['knowledge_doc', 'parent_asin'], inplace=True)\n",
    "    df['parent_asin'] = df['parent_asin'].astype(str)\n",
    "    all_documents_with_ids = list(zip(df['knowledge_doc'], df['parent_asin']))\n",
    "    print(f\"Loaded {len(all_documents_with_ids)} documents to process.\")\n",
    "\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if INDEX_NAME in pc.list_indexes().names():\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        print(f\"Index '{INDEX_NAME}' already exists. Deleting all vectors to start fresh...\")\n",
    "        index.delete(delete_all=True)\n",
    "        print(\"All vectors deleted.\")\n",
    "    else:\n",
    "        print(f\"Creating a new serverless index: {INDEX_NAME}\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=768, metric='cosine',\n",
    "            spec=ServerlessSpec(cloud=CLOUD_PROVIDER, region=CLOUD_REGION)\n",
    "        )\n",
    "\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    print(\"Pinecone initialized and ready.\")\n",
    "\n",
    "    # Create the shared resources: the queue and the new rate limiter\n",
    "    embedding_queue = asyncio.Queue(maxsize=NUM_PRODUCER_WORKERS * 2)\n",
    "    rate_limiter = AsyncRateLimiter(rate_limit=GEMINI_RPM_LIMIT, period_seconds=60)\n",
    "\n",
    "    # --- Create and start workers ---\n",
    "    consumer_tasks = [\n",
    "        asyncio.create_task(consumer_worker(i, embedding_queue, index))\n",
    "        for i in range(NUM_CONSUMER_WORKERS)\n",
    "    ]\n",
    "\n",
    "    docs_per_worker = len(all_documents_with_ids) // NUM_PRODUCER_WORKERS\n",
    "    producer_tasks = []\n",
    "    for i in range(NUM_PRODUCER_WORKERS):\n",
    "        start_index = i * docs_per_worker\n",
    "        end_index = (i + 1) * docs_per_worker if i < NUM_PRODUCER_WORKERS - 1 else len(all_documents_with_ids)\n",
    "        worker_docs = all_documents_with_ids[start_index:end_index]\n",
    "        if not worker_docs: continue\n",
    "        # Pass the shared rate limiter to each producer worker\n",
    "        task = asyncio.create_task(producer_worker(i, embedding_queue, worker_docs, rate_limiter))\n",
    "        producer_tasks.append(task)\n",
    "\n",
    "    # --- Wait for completion ---\n",
    "    await asyncio.gather(*producer_tasks)\n",
    "    print(\"\\nAll producer workers have finished.\")\n",
    "\n",
    "    # Signal consumers to shut down\n",
    "    await embedding_queue.put(None)\n",
    "\n",
    "    await asyncio.gather(*consumer_tasks)\n",
    "    print(\"All consumer workers have finished.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nPipeline finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ======================================================================================\n",
    "# 6. EXECUTE THE PIPELINE\n",
    "# ======================================================================================\n",
    "await main()\n"
   ],
   "id": "b3a2e6265f7fe3fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/knowledge_base.csv...\n",
      "Loaded 20000 documents to process.\n",
      "Initializing Pinecone...\n",
      "Index 'electronics-gemini' already exists. Deleting all vectors to start fresh...\n",
      "All vectors deleted.\n",
      "Pinecone initialized and ready.\n",
      "[Consumer-0] Starting...\n",
      "[Consumer-1] Starting...\n",
      "[Consumer-2] Starting...\n",
      "[Consumer-3] Starting...\n",
      "[Consumer-4] Starting...\n",
      "[Consumer-5] Starting...\n",
      "[Consumer-6] Starting...\n",
      "[Consumer-7] Starting...\n",
      "[Consumer-8] Starting...\n",
      "[Consumer-9] Starting...\n",
      "[Consumer-10] Starting...\n",
      "[Consumer-11] Starting...\n",
      "[Consumer-12] Starting...\n",
      "[Consumer-13] Starting...\n",
      "[Consumer-14] Starting...\n",
      "[Consumer-15] Starting...\n",
      "[Consumer-16] Starting...\n",
      "[Consumer-17] Starting...\n",
      "[Consumer-18] Starting...\n",
      "[Consumer-19] Starting...\n",
      "[Consumer-20] Starting...\n",
      "[Consumer-21] Starting...\n",
      "[Consumer-22] Starting...\n",
      "[Consumer-23] Starting...\n",
      "[Consumer-24] Starting...\n",
      "[Consumer-25] Starting...\n",
      "[Consumer-26] Starting...\n",
      "[Consumer-27] Starting...\n",
      "[Consumer-28] Starting...\n",
      "[Consumer-29] Starting...\n",
      "[Producer-0] Starting...\n",
      "[Producer-1] Starting...\n",
      "[Producer-2] Starting...\n",
      "[Producer-3] Starting...\n",
      "[Producer-4] Starting...\n",
      "[Producer-5] Starting...\n",
      "[Producer-6] Starting...\n",
      "[Producer-7] Starting...\n",
      "[Producer-8] Starting...\n",
      "[Producer-9] Starting...\n",
      "[Producer-1] Finished. Processed 2000 docs in 41.09 seconds.\n",
      "[Producer-0] Finished. Processed 2000 docs in 41.94 seconds.\n",
      "[Producer-2] Finished. Processed 2000 docs in 43.79 seconds.\n",
      "[Producer-3] Finished. Processed 2000 docs in 44.09 seconds.\n",
      "[Producer-4] Finished. Processed 2000 docs in 45.93 seconds.\n",
      "[Producer-6] Finished. Processed 2000 docs in 46.60 seconds.\n",
      "[Producer-7] Finished. Processed 2000 docs in 47.08 seconds.\n",
      "[Producer-5] Finished. Processed 2000 docs in 47.13 seconds.\n",
      "[Producer-8] Finished. Processed 2000 docs in 47.91 seconds.\n",
      "[Producer-9] Finished. Processed 2000 docs in 48.44 seconds.\n",
      "\n",
      "All producer workers have finished.\n",
      "[Consumer-1] Finished. Processed 662 vectors in 48.49 seconds.\n",
      "[Consumer-3] Finished. Processed 708 vectors in 48.51 seconds.\n",
      "[Consumer-2] Finished. Processed 683 vectors in 48.53 seconds.\n",
      "[Consumer-0] Finished. Processed 705 vectors in 48.56 seconds.\n",
      "[Consumer-6] Finished. Processed 684 vectors in 48.57 seconds.\n",
      "[Consumer-5] Finished. Processed 675 vectors in 48.59 seconds.\n",
      "[Consumer-7] Finished. Processed 691 vectors in 48.62 seconds.\n",
      "[Consumer-4] Finished. Processed 647 vectors in 48.63 seconds.\n",
      "[Consumer-8] Finished. Processed 747 vectors in 48.65 seconds.\n",
      "[Consumer-9] Finished. Processed 764 vectors in 48.67 seconds.\n",
      "[Consumer-11] Finished. Processed 711 vectors in 48.69 seconds.\n",
      "[Consumer-12] Finished. Processed 690 vectors in 48.72 seconds.\n",
      "[Consumer-10] Finished. Processed 699 vectors in 48.74 seconds.\n",
      "[Consumer-13] Finished. Processed 718 vectors in 48.75 seconds.\n",
      "[Consumer-15] Finished. Processed 687 vectors in 48.77 seconds.\n",
      "[Consumer-16] Finished. Processed 709 vectors in 48.79 seconds.\n",
      "[Consumer-14] Finished. Processed 714 vectors in 48.82 seconds.\n",
      "[Consumer-17] Finished. Processed 720 vectors in 48.83 seconds.\n",
      "[Consumer-18] Finished. Processed 655 vectors in 48.85 seconds.\n",
      "[Consumer-19] Finished. Processed 694 vectors in 48.86 seconds.\n",
      "[Consumer-21] Finished. Processed 584 vectors in 48.88 seconds.\n",
      "[Consumer-20] Finished. Processed 639 vectors in 49.16 seconds.\n",
      "[Consumer-23] Finished. Processed 627 vectors in 49.34 seconds.\n",
      "[Consumer-22] Finished. Processed 574 vectors in 49.65 seconds.\n",
      "[Consumer-24] Finished. Processed 639 vectors in 51.95 seconds.\n",
      "[Consumer-26] Finished. Processed 589 vectors in 53.34 seconds.\n",
      "[Consumer-29] Finished. Processed 542 vectors in 53.49 seconds.\n",
      "[Consumer-28] Finished. Processed 598 vectors in 53.70 seconds.\n",
      "[Consumer-27] Finished. Processed 616 vectors in 53.75 seconds.\n",
      "[Consumer-25] Finished. Processed 629 vectors in 53.79 seconds.\n",
      "All consumer workers have finished.\n",
      "\n",
      "Pipeline finished in 63.97 seconds.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T22:59:57.796788Z",
     "start_time": "2025-06-22T22:58:51.892326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.asyncio import tqdm\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# ======================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ======================================================================================\n",
    "# Make sure your .env file is in the project root.\n",
    "load_dotenv()\n",
    "\n",
    "# --- API Keys ---\n",
    "# *** NEW: Load all available Gemini API keys from the .env file ***\n",
    "GEMINI_API_KEYS = [os.getenv(key) for key in os.environ if key.startswith(\"GEMINI_API_KEY\") and os.getenv(key)]\n",
    "if not GEMINI_API_KEYS:\n",
    "    raise ValueError(\"No GEMINI_API_KEY... variables found in .env file. Please add at least one.\")\n",
    "print(f\"Found {len(GEMINI_API_KEYS)} Gemini API keys to use in the pool.\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# --- Pinecone Config ---\n",
    "INDEX_NAME = \"electronics-gemini\"\n",
    "CLOUD_PROVIDER = \"aws\"\n",
    "CLOUD_REGION = \"us-east-1\"\n",
    "MAX_PINECONE_REQUEST_SIZE_BYTES = 2 * 1024 * 1024 * 0.9\n",
    "\n",
    "# --- Gemini Config ---\n",
    "MAX_GEMINI_REQUEST_SIZE_BYTES = 1 * 1024 * 1024\n",
    "\n",
    "# --- Data Config ---\n",
    "DATA_FILE_PATH = \"../data/knowledge_base.csv\"\n",
    "NUM_DOCUMENTS_TO_PROCESS = 20000\n",
    "\n",
    "# --- Concurrency Config ---\n",
    "# As requested, we will use 10 workers for each available API key.\n",
    "WORKERS_PER_KEY = 10\n",
    "NUM_PRODUCER_WORKERS = len(GEMINI_API_KEYS) * WORKERS_PER_KEY\n",
    "NUM_CONSUMER_WORKERS = 30 # This can remain high\n",
    "\n",
    "# ======================================================================================\n",
    "# 2. HELPER FUNCTIONS FOR ADAPTIVE BATCHING\n",
    "# ======================================================================================\n",
    "\n",
    "def create_adaptive_text_batches(documents_with_ids):\n",
    "    \"\"\"\n",
    "    A generator that yields size-optimized batches of text documents for the Gemini API.\n",
    "    \"\"\"\n",
    "    current_batch_docs, current_batch_ids, current_batch_size = [], [], 0\n",
    "    for doc, doc_id in documents_with_ids:\n",
    "        doc_size = sys.getsizeof(doc)\n",
    "        if current_batch_size + doc_size > MAX_GEMINI_REQUEST_SIZE_BYTES and current_batch_docs:\n",
    "            yield current_batch_docs, current_batch_ids\n",
    "            current_batch_docs, current_batch_ids, current_batch_size = [], [], 0\n",
    "        current_batch_docs.append(doc)\n",
    "        current_batch_ids.append(doc_id)\n",
    "        current_batch_size += doc_size\n",
    "    if current_batch_docs:\n",
    "        yield current_batch_docs, current_batch_ids\n",
    "\n",
    "def create_adaptive_pinecone_batches(vectors_with_ids):\n",
    "    \"\"\"\n",
    "    A generator that yields size-optimized batches for Pinecone upsert.\n",
    "    \"\"\"\n",
    "    current_batch, current_batch_size = [], 0\n",
    "    for vec_with_id in vectors_with_ids:\n",
    "        vector_size = sys.getsizeof(vec_with_id[1]) + sys.getsizeof(vec_with_id[0])\n",
    "        if current_batch_size + vector_size > MAX_PINECONE_REQUEST_SIZE_BYTES and current_batch:\n",
    "            yield current_batch\n",
    "            current_batch, current_batch_size = [], 0\n",
    "        current_batch.append(vec_with_id)\n",
    "        current_batch_size += vector_size\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "# ======================================================================================\n",
    "# 3. ASYNC PIPELINE COMPONENTS\n",
    "# ======================================================================================\n",
    "\n",
    "async def producer_worker(worker_id: int, api_key: str, queue: asyncio.Queue, documents_with_ids: list):\n",
    "    \"\"\"\n",
    "    A single producer worker that uses its own assigned API key.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"[Producer-{worker_id}] Starting (using key ending in ...{api_key[-4:]})\")\n",
    "    # Each worker configures with its own key\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    batch_generator = create_adaptive_text_batches(documents_with_ids)\n",
    "\n",
    "    total_docs_processed = 0\n",
    "    for batch_docs, batch_ids in batch_generator:\n",
    "        try:\n",
    "            # A simple sleep per batch per worker is a safe way to respect RPM limits\n",
    "            await asyncio.sleep(1)\n",
    "            result = await genai.embed_content_async(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                content=batch_docs,\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "            )\n",
    "            embeddings_with_ids = list(zip(batch_ids, result['embedding']))\n",
    "            total_docs_processed += len(embeddings_with_ids)\n",
    "            await queue.put(embeddings_with_ids)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Producer-{worker_id}] Error: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[Producer-{worker_id}] Finished. Processed {total_docs_processed} docs in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "async def consumer_worker(worker_id: int, queue: asyncio.Queue, index: Pinecone.Index):\n",
    "    \"\"\"\n",
    "    A single consumer worker that gets embeddings from the queue and upserts them.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"[Consumer-{worker_id}] Starting...\")\n",
    "    total_vectors_processed = 0\n",
    "    while True:\n",
    "        embedding_batch = await queue.get()\n",
    "        if embedding_batch is None:\n",
    "            await queue.put(None)\n",
    "            break\n",
    "        try:\n",
    "            for i, adaptive_batch in enumerate(create_adaptive_pinecone_batches(embedding_batch)):\n",
    "                total_vectors_processed += len(adaptive_batch)\n",
    "                loop = asyncio.get_running_loop()\n",
    "                await loop.run_in_executor(None, lambda: index.upsert(vectors=adaptive_batch))\n",
    "        except Exception as e:\n",
    "            print(f\"[Consumer-{worker_id}] Error: {e}\")\n",
    "        queue.task_done()\n",
    "    end_time = time.time()\n",
    "    print(f\"[Consumer-{worker_id}] Finished. Processed {total_vectors_processed} vectors in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ======================================================================================\n",
    "# 4. MAIN ORCHESTRATOR\n",
    "# ======================================================================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function to orchestrate the multi-key, multi-worker pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Loading data from {DATA_FILE_PATH}...\")\n",
    "    df = pd.read_csv(DATA_FILE_PATH).head(NUM_DOCUMENTS_TO_PROCESS)\n",
    "    df.dropna(subset=['knowledge_doc', 'parent_asin'], inplace=True)\n",
    "    df['parent_asin'] = df['parent_asin'].astype(str)\n",
    "    all_documents_with_ids = list(zip(df['knowledge_doc'], df['parent_asin']))\n",
    "    print(f\"Loaded {len(all_documents_with_ids)} documents to process.\")\n",
    "\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if INDEX_NAME in pc.list_indexes().names():\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        index.delete(delete_all=True)\n",
    "    else:\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=768, metric='cosine',\n",
    "            spec=ServerlessSpec(cloud=CLOUD_PROVIDER, region=CLOUD_REGION)\n",
    "        )\n",
    "\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    print(\"Pinecone initialized and ready.\")\n",
    "\n",
    "    embedding_queue = asyncio.Queue(maxsize=NUM_PRODUCER_WORKERS * 2)\n",
    "\n",
    "    consumer_tasks = [\n",
    "        asyncio.create_task(consumer_worker(i, embedding_queue, index))\n",
    "        for i in range(NUM_CONSUMER_WORKERS)\n",
    "    ]\n",
    "\n",
    "    # Create a cycle of the available API keys for round-robin assignment\n",
    "    key_cycle = itertools.cycle(GEMINI_API_KEYS)\n",
    "\n",
    "    # Distribute the documents evenly among the total number of producer workers\n",
    "    docs_per_worker = len(all_documents_with_ids) // NUM_PRODUCER_WORKERS\n",
    "    producer_tasks = []\n",
    "    for i in range(NUM_PRODUCER_WORKERS):\n",
    "        start_index = i * docs_per_worker\n",
    "        end_index = (i + 1) * docs_per_worker if i < NUM_PRODUCER_WORKERS - 1 else len(all_documents_with_ids)\n",
    "        worker_docs = all_documents_with_ids[start_index:end_index]\n",
    "        if not worker_docs: continue\n",
    "\n",
    "        # Assign an API key from the pool to this worker\n",
    "        api_key = next(key_cycle)\n",
    "        task = asyncio.create_task(producer_worker(i, api_key, embedding_queue, worker_docs))\n",
    "        producer_tasks.append(task)\n",
    "\n",
    "    await asyncio.gather(*producer_tasks)\n",
    "    print(\"\\nAll producer workers have finished.\")\n",
    "\n",
    "    await embedding_queue.put(None)\n",
    "\n",
    "    await asyncio.gather(*consumer_tasks)\n",
    "    print(\"All consumer workers have finished.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nPipeline finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ======================================================================================\n",
    "# 5. EXECUTE THE PIPELINE\n",
    "# ======================================================================================\n",
    "await main()\n"
   ],
   "id": "1b3497414e2f8465",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Gemini API keys to use in the pool.\n",
      "Loading data from ../data/knowledge_base.csv...\n",
      "Loaded 20000 documents to process.\n",
      "Initializing Pinecone...\n",
      "Pinecone initialized and ready.\n",
      "[Consumer-0] Starting...\n",
      "[Consumer-1] Starting...\n",
      "[Consumer-2] Starting...\n",
      "[Consumer-3] Starting...\n",
      "[Consumer-4] Starting...\n",
      "[Consumer-5] Starting...\n",
      "[Consumer-6] Starting...\n",
      "[Consumer-7] Starting...\n",
      "[Consumer-8] Starting...\n",
      "[Consumer-9] Starting...\n",
      "[Consumer-10] Starting...\n",
      "[Consumer-11] Starting...\n",
      "[Consumer-12] Starting...\n",
      "[Consumer-13] Starting...\n",
      "[Consumer-14] Starting...\n",
      "[Consumer-15] Starting...\n",
      "[Consumer-16] Starting...\n",
      "[Consumer-17] Starting...\n",
      "[Consumer-18] Starting...\n",
      "[Consumer-19] Starting...\n",
      "[Consumer-20] Starting...\n",
      "[Consumer-21] Starting...\n",
      "[Consumer-22] Starting...\n",
      "[Consumer-23] Starting...\n",
      "[Consumer-24] Starting...\n",
      "[Consumer-25] Starting...\n",
      "[Consumer-26] Starting...\n",
      "[Consumer-27] Starting...\n",
      "[Consumer-28] Starting...\n",
      "[Consumer-29] Starting...\n",
      "[Producer-0] Starting (using key ending in ...rfnA)\n",
      "[Producer-1] Starting (using key ending in ...To2s)\n",
      "[Producer-2] Starting (using key ending in ...rfnA)\n",
      "[Producer-3] Starting (using key ending in ...To2s)\n",
      "[Producer-4] Starting (using key ending in ...rfnA)\n",
      "[Producer-5] Starting (using key ending in ...To2s)\n",
      "[Producer-6] Starting (using key ending in ...rfnA)\n",
      "[Producer-7] Starting (using key ending in ...To2s)\n",
      "[Producer-8] Starting (using key ending in ...rfnA)\n",
      "[Producer-9] Starting (using key ending in ...To2s)\n",
      "[Producer-10] Starting (using key ending in ...rfnA)\n",
      "[Producer-11] Starting (using key ending in ...To2s)\n",
      "[Producer-12] Starting (using key ending in ...rfnA)\n",
      "[Producer-13] Starting (using key ending in ...To2s)\n",
      "[Producer-14] Starting (using key ending in ...rfnA)\n",
      "[Producer-15] Starting (using key ending in ...To2s)\n",
      "[Producer-16] Starting (using key ending in ...rfnA)\n",
      "[Producer-17] Starting (using key ending in ...To2s)\n",
      "[Producer-18] Starting (using key ending in ...rfnA)\n",
      "[Producer-19] Starting (using key ending in ...To2s)\n",
      "[Producer-15] Finished. Processed 1000 docs in 39.71 seconds.\n",
      "[Producer-13] Finished. Processed 1000 docs in 39.93 seconds.\n",
      "[Producer-11] Finished. Processed 1000 docs in 40.05 seconds.\n",
      "[Producer-12] Finished. Processed 1000 docs in 40.41 seconds.\n",
      "[Producer-7] Finished. Processed 1000 docs in 40.67 seconds.\n",
      "[Producer-10] Finished. Processed 1000 docs in 41.56 seconds.\n",
      "[Producer-6] Finished. Processed 1000 docs in 41.89 seconds.\n",
      "[Producer-18] Finished. Processed 1000 docs in 41.92 seconds.\n",
      "[Producer-4] Finished. Processed 1000 docs in 42.02 seconds.\n",
      "[Producer-8] Finished. Processed 1000 docs in 42.07 seconds.\n",
      "[Producer-19] Finished. Processed 1000 docs in 42.25 seconds.\n",
      "[Producer-3] Finished. Processed 1000 docs in 42.47 seconds.\n",
      "[Producer-14] Finished. Processed 1000 docs in 42.50 seconds.\n",
      "[Producer-9] Finished. Processed 1000 docs in 42.99 seconds.\n",
      "[Producer-16] Finished. Processed 1000 docs in 43.48 seconds.\n",
      "[Producer-1] Finished. Processed 1000 docs in 43.94 seconds.\n",
      "[Producer-0] Finished. Processed 1000 docs in 44.57 seconds.\n",
      "[Producer-5] Finished. Processed 1000 docs in 44.88 seconds.\n",
      "[Producer-17] Finished. Processed 1000 docs in 45.03 seconds.\n",
      "[Producer-2] Finished. Processed 1000 docs in 45.19 seconds.\n",
      "\n",
      "All producer workers have finished.\n",
      "[Consumer-12] Finished. Processed 668 vectors in 45.47 seconds.\n",
      "[Consumer-11] Finished. Processed 662 vectors in 45.49 seconds.\n",
      "[Consumer-14] Finished. Processed 698 vectors in 45.49 seconds.\n",
      "[Consumer-10] Finished. Processed 706 vectors in 45.51 seconds.\n",
      "[Consumer-13] Finished. Processed 708 vectors in 45.53 seconds.\n",
      "[Consumer-21] Finished. Processed 264 vectors in 46.49 seconds.\n",
      "[Consumer-17] Finished. Processed 705 vectors in 48.62 seconds.\n",
      "[Consumer-27] Finished. Processed 236 vectors in 49.08 seconds.\n",
      "[Consumer-16] Finished. Processed 733 vectors in 49.78 seconds.\n",
      "[Consumer-5] Finished. Processed 940 vectors in 49.99 seconds.\n",
      "[Consumer-20] Finished. Processed 289 vectors in 51.06 seconds.\n",
      "[Consumer-22] Finished. Processed 294 vectors in 51.23 seconds.\n",
      "[Consumer-1] Finished. Processed 951 vectors in 51.48 seconds.\n",
      "[Consumer-25] Finished. Processed 325 vectors in 52.07 seconds.\n",
      "[Consumer-28] Finished. Processed 292 vectors in 52.16 seconds.\n",
      "[Consumer-23] Finished. Processed 316 vectors in 52.18 seconds.\n",
      "[Consumer-15] Finished. Processed 715 vectors in 52.23 seconds.\n",
      "[Consumer-0] Finished. Processed 955 vectors in 52.37 seconds.\n",
      "[Consumer-24] Finished. Processed 330 vectors in 52.53 seconds.\n",
      "[Consumer-19] Finished. Processed 747 vectors in 52.87 seconds.\n",
      "[Consumer-4] Finished. Processed 960 vectors in 52.99 seconds.\n",
      "[Consumer-3] Finished. Processed 1000 vectors in 53.00 seconds.\n",
      "[Consumer-18] Finished. Processed 764 vectors in 53.14 seconds.\n",
      "[Consumer-26] Finished. Processed 317 vectors in 53.14 seconds.\n",
      "[Consumer-6] Finished. Processed 978 vectors in 53.43 seconds.\n",
      "[Consumer-29] Finished. Processed 353 vectors in 53.64 seconds.\n",
      "[Consumer-2] Finished. Processed 1023 vectors in 53.68 seconds.\n",
      "[Consumer-9] Finished. Processed 1038 vectors in 54.01 seconds.\n",
      "[Consumer-8] Finished. Processed 984 vectors in 54.58 seconds.\n",
      "[Consumer-7] Finished. Processed 1049 vectors in 54.70 seconds.\n",
      "All consumer workers have finished.\n",
      "\n",
      "Pipeline finished in 65.81 seconds.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8cd4bb4e7c335ddd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
