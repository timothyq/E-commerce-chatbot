{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-29T01:13:05.164815Z",
     "start_time": "2025-06-29T01:13:04.835367Z"
    }
   },
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import ast  # Import the ast module\n",
    "\n",
    "# Setup for progress bars in pandas\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T01:13:09.820045Z",
     "start_time": "2025-06-29T01:13:05.203527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The qa_Electronics.json file uses single quotes, so we'll parse it line by line\n",
    "# using ast.literal_eval, which safely evaluates it as a Python dictionary.\n",
    "\n",
    "qa_data_path = '../data/qa_Electronics.json'\n",
    "qa_asins = set()\n",
    "\n",
    "with open(qa_data_path, 'r') as f:\n",
    "    for line in tqdm(f, desc=\"Parsing qa_Electronics.json\"):\n",
    "        try:\n",
    "            # Use ast.literal_eval to parse the string as a Python dictionary\n",
    "            data = ast.literal_eval(line)\n",
    "            if 'asin' in data:\n",
    "                qa_asins.add(data['asin'])\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            # This will catch any lines that are not valid Python literals\n",
    "            print(f\"Skipping line due to error: {e}\")\n",
    "\n",
    "print(f\"Found {len(qa_asins)} unique ASINs in {qa_data_path}\")"
   ],
   "id": "52cd19bdefbf397b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parsing qa_Electronics.json: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3844d29db154abd846d7f6ab520aac3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39371 unique ASINs in ../data/qa_Electronics.json\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T01:13:23.428617Z",
     "start_time": "2025-06-29T01:13:09.827892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This file is also in a JSON-lines format, with one object per line.\n",
    "# We will read it line-by-line and parse each line individually using ast.literal_eval.\n",
    "\n",
    "qa_1_to_many_path = '../data/QA_Electronics_1_to_many.json'\n",
    "qa_1_to_many_asins = set()\n",
    "\n",
    "with open(qa_1_to_many_path, 'r') as f:\n",
    "    for line in tqdm(f, desc=\"Parsing QA_Electronics_1_to_many.json\"):\n",
    "        try:\n",
    "            # Use ast.literal_eval to parse each line as a Python dictionary\n",
    "            data = ast.literal_eval(line)\n",
    "            if 'asin' in data:\n",
    "                qa_1_to_many_asins.add(data['asin'])\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            # This will catch any lines that are not valid Python literals\n",
    "            print(f\"Skipping line due to error: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Found {len(qa_1_to_many_asins)} unique ASINs in {qa_1_to_many_path}\")"
   ],
   "id": "5565fa4e179cb611",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parsing QA_Electronics_1_to_many.json: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d85076cb960e47e9b8f621ce6879ac56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38959 unique ASINs in ../data/QA_Electronics_1_to_many.json\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T01:13:23.529185Z",
     "start_time": "2025-06-29T01:13:23.465417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables to find the cache path\n",
    "load_dotenv()\n",
    "cache_directory = os.getenv(\"HF_HOME\", \"../.cache\")\n",
    "\n",
    "print(f\"Checking for and removing .lock files in cache: {cache_directory}\")\n",
    "\n",
    "# Construct a pattern to find all .lock files recursively\n",
    "# The '**' pattern searches through all subdirectories\n",
    "lock_file_pattern = os.path.join(cache_directory, '**', '*.lock')\n",
    "\n",
    "# Find all files matching the pattern\n",
    "lock_files = glob.glob(lock_file_pattern, recursive=True)\n",
    "\n",
    "if not lock_files:\n",
    "    print(\"No .lock files found. Cache is clean.\")\n",
    "else:\n",
    "    for lock_file in lock_files:\n",
    "        try:\n",
    "            os.remove(lock_file)\n",
    "            print(f\"Removed lock file: {lock_file}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error removing file {lock_file}: {e}\")\n",
    "    print(\"Lock cleaning complete.\")"
   ],
   "id": "e2be1d2805508850",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for and removing .lock files in cache: /Volumes/ExtremeSSD/workingspace/ChatBotAmazon/data\n",
      "Removed lock file: /Volumes/ExtremeSSD/workingspace/ChatBotAmazon/data/_Volumes_ExtremeSSD_workingspace_ChatBotAmazon_data_McAuley-Lab___amazon-reviews-2023_raw_review_Electronics_0.0.0_16b76e0823d73bb8cff1e9c5e3e37dbc46ae3daee380417ae141f5e67d3ea8e8.lock\n",
      "Removed lock file: /Volumes/ExtremeSSD/workingspace/ChatBotAmazon/data/McAuley-Lab___amazon-reviews-2023/raw_review_Electronics/0.0.0/16b76e0823d73bb8cff1e9c5e3e37dbc46ae3daee380417ae141f5e67d3ea8e8_builder.lock\n",
      "Removed lock file: /Volumes/ExtremeSSD/workingspace/ChatBotAmazon/data/McAuley-Lab___amazon-reviews-2023/raw_review_Electronics/0.0.0/16b76e0823d73bb8cff1e9c5e3e37dbc46ae3daee380417ae141f5e67d3ea8e8.incomplete_info.lock\n",
      "Lock cleaning complete.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T01:14:39.837537Z",
     "start_time": "2025-06-29T01:13:23.554036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Use the HF_HOME environment variable for the cache directory\n",
    "cache_directory = os.getenv(\"HF_HOME\", \"../.cache\")\n",
    "print(f\"Using Hugging Face cache directory: {cache_directory}\")\n",
    "\n",
    "# --- Part 1: Process Reviews by loading the full dataset (from cache if available) ---\n",
    "\n",
    "print(\"\\nExtracting ASINs from reviews (loading full dataset)...\")\n",
    "reviews_asins = set()\n",
    "try:\n",
    "    print(\"Loading review dataset (this will be fast if cached)...\")\n",
    "    reviews_dataset = load_dataset(\n",
    "        \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "        \"raw_review_Electronics\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_directory\n",
    "    )['full']\n",
    "\n",
    "    print(\"Extracting unique ASINs from reviews...\")\n",
    "    reviews_asins = set(reviews_dataset['asin'])\n",
    "\n",
    "    print(f\"Found {len(reviews_asins)} unique ASINs in reviews.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing reviews: {e}\")\n",
    "    reviews_asins = None\n",
    "\n",
    "\n",
    "# --- Part 2: Process Metadata by dynamically finding the file and reading from cache ---\n",
    "\n",
    "print(\"\\nExtracting ASINs from metadata (using direct Parquet read from cache)...\")\n",
    "meta_asins = set()\n",
    "try:\n",
    "    # Step A: Find the correct file path instead of hardcoding it.\n",
    "    print(\"Discovering the correct metadata file path in the repository...\")\n",
    "    repo_id = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "    all_files = list_repo_files(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "    # Filter to find the parquet file within the raw_meta_Electronics directory\n",
    "    meta_filename = next((f for f in all_files if \"raw_meta_Electronics\" in f and f.endswith('.parquet')), None)\n",
    "\n",
    "    if not meta_filename:\n",
    "        raise FileNotFoundError(\"Could not dynamically find the Parquet file for raw_meta_Electronics.\")\n",
    "\n",
    "    print(f\"Found metadata file: {meta_filename}\")\n",
    "\n",
    "    # Step B: Download (if not cached) and get the local path.\n",
    "    parquet_file_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        filename=meta_filename,\n",
    "        cache_dir=cache_directory,\n",
    "    )\n",
    "    print(f\"Reading from local file: {parquet_file_path}\")\n",
    "\n",
    "    # Step C: Read only the 'parent_asin' column from the local Parquet file.\n",
    "    parquet_file = pq.ParquetFile(parquet_file_path)\n",
    "    for i in tqdm(range(parquet_file.num_row_groups), desc=\"Reading metadata from cache\"):\n",
    "        row_group = parquet_file.read_row_group(i, columns=['parent_asin'])\n",
    "        meta_asins.update(row_group.column('parent_asin').unique().to_pylist())\n",
    "\n",
    "    print(f\"Found {len(meta_asins)} unique ASINs in metadata.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing metadata: {e}\")\n",
    "    meta_asins = None\n",
    "\n",
    "# --- Combine ASINs ---\n",
    "if meta_asins is not None and reviews_asins is not None:\n",
    "    all_huggingface_asins = meta_asins.union(reviews_asins)\n",
    "    print(f\"\\nFound {len(all_huggingface_asins)} unique ASINs in total from the Hugging Face datasets.\")\n",
    "else:\n",
    "    all_huggingface_asins = set()\n",
    "    print(\"\\nCould not process one or both Hugging Face datasets. The set of Hugging Face ASINs will be empty.\")"
   ],
   "id": "a7ad0edfe6101add",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hugging Face cache directory: /Volumes/ExtremeSSD/workingspace/ChatBotAmazon/data\n",
      "\n",
      "Extracting ASINs from reviews (loading full dataset)...\n",
      "Loading review dataset (this will be fast if cached)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/34 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a474b3aac8484c179b196601dfd3bdff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique ASINs from reviews...\n",
      "Found 1946161 unique ASINs in reviews.\n",
      "\n",
      "Extracting ASINs from metadata (using direct Parquet read from cache)...\n",
      "Discovering the correct metadata file path in the repository...\n",
      "Found metadata file: raw_meta_Electronics/full-00000-of-00010.parquet\n",
      "Reading from local file: /Volumes/ExtremeSSD/workingspace/ChatBotAmazon/data/datasets--McAuley-Lab--Amazon-Reviews-2023/snapshots/2b6d039ed471f2ba5fd2acb718bf33b0a7e5598e/raw_meta_Electronics/full-00000-of-00010.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reading metadata from cache:   0%|          | 0/162 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebf324bd1b66424c9f27840c53767d3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161002 unique ASINs in metadata.\n",
      "\n",
      "Found 1980815 unique ASINs in total from the Hugging Face datasets.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T01:14:42.296091Z",
     "start_time": "2025-06-29T01:14:42.284318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# You have these variables from the previous cells:\n",
    "# qa_asins: ASINs from qa_Electronics.json\n",
    "# qa_1_to_many_asins: ASINs from QA_Electronics_1_to_many.json\n",
    "# all_huggingface_asins: Combined ASINs from meta and reviews\n",
    "\n",
    "print(\"--- Verifying Overlap ---\")\n",
    "print(f\"Unique ASINs in qa_Electronics.json: {len(qa_asins)}\")\n",
    "print(f\"Unique ASINs in QA_Electronics_1_to_many.json: {len(qa_1_to_many_asins)}\")\n",
    "print(f\"Unique ASINs in Hugging Face (meta & reviews): {len(all_huggingface_asins)}\")\n",
    "\n",
    "# First, find the ASINs that are common to both Q&A files\n",
    "common_qa_asins = qa_asins.intersection(qa_1_to_many_asins)\n",
    "print(f\"\\nASINs found in BOTH Q&A files: {len(common_qa_asins)}\")\n",
    "\n",
    "# Now, find the final intersection with the products from Hugging Face\n",
    "final_intersection_asins = common_qa_asins.intersection(all_huggingface_asins)\n",
    "\n",
    "print(f\"----------------------------------------------------\")\n",
    "print(f\"Final number of products with rich data (meta, reviews, and both Q&As): {len(final_intersection_asins)}\")\n",
    "print(\"These are the products we will use to build the final dataset.\")"
   ],
   "id": "bde27eae04ed45df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Overlap ---\n",
      "Unique ASINs in qa_Electronics.json: 39371\n",
      "Unique ASINs in QA_Electronics_1_to_many.json: 38959\n",
      "Unique ASINs in Hugging Face (meta & reviews): 1980815\n",
      "\n",
      "ASINs found in BOTH Q&A files: 38959\n",
      "----------------------------------------------------\n",
      "Final number of products with rich data (meta, reviews, and both Q&As): 29178\n",
      "These are the products we will use to build the final dataset.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T08:17:09.446793Z",
     "start_time": "2025-06-29T07:35:28.463190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Setup and Configuration ---\n",
    "print(\"--- Step 1: Initializing Setup ---\")\n",
    "load_dotenv()\n",
    "cache_directory = os.getenv(\"HF_HOME\", \"../.cache\")\n",
    "qa_file_1 = '../data/qa_Electronics.json'\n",
    "qa_file_2 = '../data/QA_Electronics_1_to_many.json'\n",
    "repo_id = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "output_dir = \"../data/gold_standard_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Final data will be saved in: {output_dir}\")\n",
    "\n",
    "# --- 2. Get ASINs from ALL Original Sources ---\n",
    "print(\"\\n--- Step 2: Extracting ASINs from all original sources ---\")\n",
    "\n",
    "# Get ASINs from Q&A files (with cleaning)\n",
    "def get_asins_from_qa_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return {str(ast.literal_eval(line).get('asin')).strip() for line in f}\n",
    "qa1_asins = get_asins_from_qa_file(qa_file_1)\n",
    "qa2_asins = get_asins_from_qa_file(qa_file_2)\n",
    "qa_asins = qa1_asins.union(qa2_asins) # Combine all Q&A ASINs\n",
    "print(f\"Found {len(qa_asins)} unique ASINs in all Q&A files.\")\n",
    "\n",
    "# Get ASINs from the full reviews dataset\n",
    "print(\"Loading review ASINs...\")\n",
    "reviews_dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Electronics\", trust_remote_code=True, cache_dir=cache_directory)['full']\n",
    "# Use the efficient .unique() method and clean the ASINs\n",
    "reviews_asins = {str(asin).strip() for asin in reviews_dataset.unique('asin')}\n",
    "print(f\"Found {len(reviews_asins)} unique ASINs in reviews dataset.\")\n",
    "\n",
    "# Get ASINs from all 10 metadata shards\n",
    "print(\"Loading metadata ASINs...\")\n",
    "meta_asins = set()\n",
    "for i in tqdm(range(10), desc=\"Scanning meta shards for ASINs\"):\n",
    "    shard_filename = f\"raw_meta_Electronics/full-{i:05d}-of-00010.parquet\"\n",
    "    local_path = hf_hub_download(repo_id, filename=shard_filename, repo_type=\"dataset\", cache_dir=cache_directory)\n",
    "    # Read only the 'parent_asin' column for efficiency\n",
    "    asin_column = pq.read_table(local_path, columns=['parent_asin']).to_pandas()['parent_asin']\n",
    "    meta_asins.update(asin_column.astype(str).str.strip())\n",
    "print(f\"Found {len(meta_asins)} unique ASINs in metadata.\")\n",
    "\n",
    "# --- 3. Calculate the TRUE, STRICT Intersection ---\n",
    "print(\"\\n--- Step 3: Calculating the strict intersection of all sources ---\")\n",
    "gold_standard_asins = list(qa_asins.intersection(reviews_asins).intersection(meta_asins))\n",
    "print(f\"Found {len(gold_standard_asins)} products that exist in ALL three original datasets.\")\n",
    "\n",
    "# --- 4. Filter Original Data Sources ONCE with the Gold Standard Set ---\n",
    "print(\"\\n--- Step 4: Filtering original data sources with the gold standard set ---\")\n",
    "\n",
    "# Filter Metadata\n",
    "print(\"Filtering metadata...\")\n",
    "meta_df_list = []\n",
    "for i in tqdm(range(10), desc=\"Filtering meta shards\"):\n",
    "    shard_filename = f\"raw_meta_Electronics/full-{i:05d}-of-00010.parquet\"\n",
    "    local_path = hf_hub_download(repo_id, filename=shard_filename, repo_type=\"dataset\", cache_dir=cache_directory)\n",
    "    # Filter the data on disk before loading into pandas\n",
    "    table = pq.read_table(local_path, filters=[('parent_asin', 'in', gold_standard_asins)])\n",
    "    meta_df_list.append(table.to_pandas())\n",
    "gold_meta_df = pd.concat(meta_df_list, ignore_index=True)\n",
    "print(f\"Final metadata count: {len(gold_meta_df)}\")\n",
    "\n",
    "\n",
    "# Filter Reviews\n",
    "print(\"Filtering reviews...\")\n",
    "filtered_reviews_dataset = reviews_dataset.filter(\n",
    "    lambda example: str(example[\"asin\"]).strip() in gold_standard_asins,\n",
    "    num_proc=4\n",
    ")\n",
    "gold_reviews_df = filtered_reviews_dataset.to_pandas()\n",
    "print(f\"Final reviews count: {len(gold_reviews_df)}\")\n",
    "\n",
    "\n",
    "# Filter Q&A\n",
    "print(\"Filtering Q&A...\")\n",
    "def load_and_filter_qa(path, asins_to_keep):\n",
    "    with open(path, 'r') as f:\n",
    "        records = [ast.literal_eval(line) for line in f if str(ast.literal_eval(line).get('asin')).strip() in asins_to_keep]\n",
    "    return pd.DataFrame(records)\n",
    "qa_df1 = load_and_filter_qa(qa_file_1, gold_standard_asins)\n",
    "qa_df2 = load_and_filter_qa(qa_file_2, gold_standard_asins)\n",
    "gold_qa_df = pd.concat([qa_df1, qa_df2], ignore_index=True)\n",
    "print(f\"Final Q&A count: {len(gold_qa_df)}\")\n",
    "\n"
   ],
   "id": "b2fded8ee863c956",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Initializing Setup ---\n",
      "Final data will be saved in: ../data/gold_standard_data\n",
      "\n",
      "--- Step 2: Extracting ASINs from all original sources ---\n",
      "Found 39371 unique ASINs in all Q&A files.\n",
      "Loading review ASINs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/34 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "158698a244784464bdd5b365156683db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1946161 unique ASINs in reviews dataset.\n",
      "Loading metadata ASINs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Scanning meta shards for ASINs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "833124650537436fbb5af845c9728583"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1610012 unique ASINs in metadata.\n",
      "\n",
      "--- Step 3: Calculating the strict intersection of all sources ---\n",
      "Found 22974 products that exist in ALL three original datasets.\n",
      "\n",
      "--- Step 4: Filtering original data sources with the gold standard set ---\n",
      "Filtering metadata...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filtering meta shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60d1ceb24bf64dbb9bec6ef3576a0cfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final metadata count: 22974\n",
      "Filtering reviews...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/43886944 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfdf7bb4c2f64b65a63f1c6def6687ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reviews count: 3473796\n",
      "Filtering Q&A...\n",
      "Final Q&A count: 203738\n",
      "\n",
      "--- Step 5: Saving the final 'gold standard' files ---\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'utf-8' codec can't encode character '\\ude03' in position 71: surrogates not allowed",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnicodeEncodeError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 98\u001B[39m\n\u001B[32m     96\u001B[39m gold_meta_df.to_parquet(os.path.join(output_dir, \u001B[33m\"\u001B[39m\u001B[33mgold_metadata.parquet\u001B[39m\u001B[33m\"\u001B[39m), index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     97\u001B[39m gold_reviews_df.to_parquet(os.path.join(output_dir, \u001B[33m\"\u001B[39m\u001B[33mgold_reviews.parquet\u001B[39m\u001B[33m\"\u001B[39m), index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m \u001B[43mgold_qa_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgold_qa.parquet\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mProcess complete. You now have three perfectly aligned Parquet files.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001B[39m, in \u001B[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) > num_allow_args:\n\u001B[32m    328\u001B[39m     warnings.warn(\n\u001B[32m    329\u001B[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001B[32m    330\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    331\u001B[39m         stacklevel=find_stack_level(),\n\u001B[32m    332\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3113\u001B[39m, in \u001B[36mDataFrame.to_parquet\u001B[39m\u001B[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001B[39m\n\u001B[32m   3032\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3033\u001B[39m \u001B[33;03mWrite a DataFrame to the binary parquet format.\u001B[39;00m\n\u001B[32m   3034\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   3109\u001B[39m \u001B[33;03m>>> content = f.read()\u001B[39;00m\n\u001B[32m   3110\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3111\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mio\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mparquet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m to_parquet\n\u001B[32m-> \u001B[39m\u001B[32m3113\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mto_parquet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3114\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   3115\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3116\u001B[39m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3117\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3118\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3119\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpartition_cols\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpartition_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3120\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3121\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3122\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:480\u001B[39m, in \u001B[36mto_parquet\u001B[39m\u001B[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001B[39m\n\u001B[32m    476\u001B[39m impl = get_engine(engine)\n\u001B[32m    478\u001B[39m path_or_buf: FilePath | WriteBuffer[\u001B[38;5;28mbytes\u001B[39m] = io.BytesIO() \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m path\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m \u001B[43mimpl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpartition_cols\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpartition_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    487\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    489\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    492\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, io.BytesIO)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:190\u001B[39m, in \u001B[36mPyArrowImpl.write\u001B[39m\u001B[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    188\u001B[39m     from_pandas_kwargs[\u001B[33m\"\u001B[39m\u001B[33mpreserve_index\u001B[39m\u001B[33m\"\u001B[39m] = index\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m table = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfrom_pandas_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m df.attrs:\n\u001B[32m    193\u001B[39m     df_metadata = {\u001B[33m\"\u001B[39m\u001B[33mPANDAS_ATTRS\u001B[39m\u001B[33m\"\u001B[39m: json.dumps(df.attrs)}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pyarrow/table.pxi:4793\u001B[39m, in \u001B[36mpyarrow.lib.Table.from_pandas\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pyarrow/pandas_compat.py:652\u001B[39m, in \u001B[36mdataframe_to_arrays\u001B[39m\u001B[34m(df, schema, preserve_index, nthreads, columns, safe)\u001B[39m\n\u001B[32m    650\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, maybe_fut \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(arrays):\n\u001B[32m    651\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_fut, futures.Future):\n\u001B[32m--> \u001B[39m\u001B[32m652\u001B[39m             arrays[i] = \u001B[43mmaybe_fut\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    654\u001B[39m types = [x.type \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[32m    656\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    447\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/thread.py:58\u001B[39m, in \u001B[36m_WorkItem.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     55\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m     60\u001B[39m     \u001B[38;5;28mself\u001B[39m.future.set_exception(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pyarrow/pandas_compat.py:620\u001B[39m, in \u001B[36mdataframe_to_arrays.<locals>.convert_column\u001B[39m\u001B[34m(col, field)\u001B[39m\n\u001B[32m    617\u001B[39m     type_ = field.type\n\u001B[32m    619\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m     result = \u001B[43mpa\u001B[49m\u001B[43m.\u001B[49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtype_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_pandas\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe\u001B[49m\u001B[43m=\u001B[49m\u001B[43msafe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    621\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (pa.ArrowInvalid,\n\u001B[32m    622\u001B[39m         pa.ArrowNotImplementedError,\n\u001B[32m    623\u001B[39m         pa.ArrowTypeError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    624\u001B[39m     e.args += (\u001B[33m\"\u001B[39m\u001B[33mConversion failed for column \u001B[39m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[33m with type \u001B[39m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    625\u001B[39m                .format(col.name, col.dtype),)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pyarrow/array.pxi:365\u001B[39m, in \u001B[36mpyarrow.lib.array\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pyarrow/array.pxi:90\u001B[39m, in \u001B[36mpyarrow.lib._ndarray_to_array\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/e-commerce-chatbot/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:89\u001B[39m, in \u001B[36mpyarrow.lib.check_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mUnicodeEncodeError\u001B[39m: 'utf-8' codec can't encode character '\\ude03' in position 71: surrogates not allowed"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:11:55.533782Z",
     "start_time": "2025-06-29T19:11:30.123671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This cell assumes the previous steps have created:\n",
    "# - gold_meta_df (DataFrame)\n",
    "# - gold_reviews_df (DataFrame)\n",
    "# - gold_qa_df (DataFrame)\n",
    "# - output_dir (string path)\n",
    "\n",
    "print(\"\\n--- Step 5: Cleaning and Saving the final 'gold standard' files ---\")\n",
    "\n",
    "# --- Clean the Q&A DataFrame for Unicode errors ---\n",
    "print(\"Cleaning text data in the Q&A DataFrame...\")\n",
    "\n",
    "def clean_surrogates(text):\n",
    "    \"\"\"\n",
    "    Cleans invalid surrogate characters from a string to ensure UTF-8 compatibility.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # The 'surrogatepass' error handler correctly handles these broken characters\n",
    "    return text.encode('utf-8', 'surrogatepass').decode('utf-8')\n",
    "\n",
    "# Apply the cleaning function to all object (likely text) columns in the Q&A data\n",
    "for col in gold_qa_df.select_dtypes(include=['object']).columns:\n",
    "    gold_qa_df[col] = gold_qa_df[col].astype(str).apply(clean_surrogates)\n",
    "\n",
    "print(\"Q&A data has been cleaned.\")\n",
    "\n",
    "\n",
    "# --- Save the final, consistent DataFrames ---\n",
    "try:\n",
    "    print(\"Saving the final, clean files...\")\n",
    "    # Save the metadata (which was already clean)\n",
    "    gold_meta_df.to_parquet(os.path.join(output_dir, \"gold_metadata.parquet\"), index=False)\n",
    "\n",
    "    # Save the reviews (which was also clean)\n",
    "    gold_reviews_df.to_parquet(os.path.join(output_dir, \"gold_reviews.parquet\"), index=False)\n",
    "\n",
    "    # Save the now-clean Q&A data\n",
    "    gold_qa_df.to_parquet(os.path.join(output_dir, \"gold_qa.parquet\"), index=False)\n",
    "\n",
    "    print(\"\\n✅ Process complete. You now have three perfectly aligned and clean Parquet files.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during saving: {e}\")"
   ],
   "id": "b6d927d7a8780c5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Cleaning and Saving the final 'gold standard' files ---\n",
      "Cleaning text data in the Q&A DataFrame...\n",
      "Q&A data has been cleaned.\n",
      "Saving the final, clean files...\n",
      "\n",
      "✅ Process complete. You now have three perfectly aligned and clean Parquet files.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:14:19.868818Z",
     "start_time": "2025-06-29T19:14:09.115137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Loading Gold Standard Files ---\")\n",
    "\n",
    "# Define the path to your final, clean data\n",
    "data_dir = \"../data/gold_standard_data\"\n",
    "\n",
    "# Load each of the final, consistent Parquet files\n",
    "meta_df = pd.read_parquet(os.path.join(data_dir, \"gold_metadata.parquet\"))\n",
    "reviews_df = pd.read_parquet(os.path.join(data_dir, \"gold_reviews.parquet\"))\n",
    "qa_df = pd.read_parquet(os.path.join(data_dir, \"gold_qa.parquet\"))\n",
    "\n",
    "print(f\"Loaded {len(meta_df)} metadata records.\")\n",
    "print(f\"Loaded {len(reviews_df)} review records.\")\n",
    "print(f\"Loaded {len(qa_df)} Q&A records.\")"
   ],
   "id": "b8fda95566f6a830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Gold Standard Files ---\n",
      "Loaded 22974 metadata records.\n",
      "Loaded 3473796 review records.\n",
      "Loaded 203738 Q&A records.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:14:53.404094Z",
     "start_time": "2025-06-29T19:14:34.342259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Aggregating and Merging Data ---\")\n",
    "\n",
    "# 1. Aggregate reviews: Group by 'asin' and create a list of all review data for each product.\n",
    "print(\"Aggregating reviews...\")\n",
    "# Using .apply(list) after grouping is a standard way to create these lists of records.\n",
    "reviews_agg = (reviews_df.groupby('asin')\n",
    "               .apply(lambda x: x.to_dict(orient='records'))\n",
    "               .reset_index(name='reviews'))\n",
    "print(f\"Aggregated reviews for {len(reviews_agg)} unique products.\")\n",
    "\n",
    "\n",
    "# 2. Aggregate Q&A data in the same way.\n",
    "print(\"Aggregating Q&A data...\")\n",
    "qa_agg = (qa_df.groupby('asin')\n",
    "          .apply(lambda x: x.to_dict(orient='records'))\n",
    "          .reset_index(name='q_and_a'))\n",
    "print(f\"Aggregated Q&A for {len(qa_agg)} unique products.\")\n",
    "\n",
    "\n",
    "# 3. Merge into the final master DataFrame\n",
    "print(\"Merging all data sources...\")\n",
    "# Start with the metadata as our base\n",
    "master_df = meta_df.copy()\n",
    "# Rename the metadata's 'parent_asin' to 'asin' for a clean merge\n",
    "master_df.rename(columns={'parent_asin': 'asin'}, inplace=True)\n",
    "\n",
    "# Merge the aggregated reviews\n",
    "master_df = pd.merge(master_df, reviews_agg, on='asin', how='left')\n",
    "\n",
    "# Merge the aggregated Q&A data\n",
    "master_df = pd.merge(master_df, qa_agg, on='asin', how='left')\n",
    "\n",
    "\n",
    "print(\"\\n--- Merge Complete ---\")\n",
    "print(f\"Final master dataset has {len(master_df)} rows (products).\")\n",
    "print(\"Columns:\", master_df.columns.tolist())\n",
    "print(\"\\nSample of the final master dataset:\")\n",
    "display(master_df.head())"
   ],
   "id": "34925a70a4a34add",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregating and Merging Data ---\n",
      "Aggregating reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/73tgs6wx3sjfczgbz38g01840000gn/T/ipykernel_31275/3439470143.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.to_dict(orient='records'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated reviews for 22974 unique products.\n",
      "Aggregating Q&A data...\n",
      "Aggregated Q&A for 22974 unique products.\n",
      "Merging all data sources...\n",
      "\n",
      "--- Merge Complete ---\n",
      "Final master dataset has 22974 rows (products).\n",
      "Columns: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'details', 'asin', 'bought_together', 'subtitle', 'author', 'reviews', 'q_and_a']\n",
      "\n",
      "Sample of the final master dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/73tgs6wx3sjfczgbz38g01840000gn/T/ipykernel_31275/3439470143.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.to_dict(orient='records'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  main_category  \\\n",
       "0  Portable Audio & Accessories   \n",
       "1                Camera & Photo   \n",
       "2                Camera & Photo   \n",
       "3                     Computers   \n",
       "4                     Computers   \n",
       "\n",
       "                                               title  average_rating  \\\n",
       "0  Dock Audio Extender Adapter Converter Cable fo...             3.6   \n",
       "1  GGS Swivi HD DSLR LCD Universal Foldable Viewf...             3.7   \n",
       "2  BM Premium 2-Pack of NP-85 Batteries and Charg...             4.7   \n",
       "3  ZAGG InvisibleShield HD – EZ Apply Film Screen...             4.1   \n",
       "4  Mobile Edge ECO Laptop Messenger (Eco-Friendly...             3.5   \n",
       "\n",
       "   rating_number                                           features  \\\n",
       "0             45                                                 []   \n",
       "1             52                                                 []   \n",
       "2            345  [NP85 Li-ion Battery for FujiFilm FinePix SL24...   \n",
       "3            158  [100% Clear: Independent light transmission te...   \n",
       "4             12  [Fits 17.3-Inch laptops. Laptop compartment di...   \n",
       "\n",
       "                                         description  price  \\\n",
       "0                                                 []   None   \n",
       "1                                                 []  149.0   \n",
       "2  [Bring your digital camera back to life with a...  22.99   \n",
       "3  [Exceptionally clear, unbelievably thin, and v...   None   \n",
       "4  [The 17.3-Inch ECO Messenger is the latest add...   None   \n",
       "\n",
       "                                              images  \\\n",
       "0  {'hi_res': [None, None], 'large': ['https://m....   \n",
       "1  {'hi_res': [None, None, None, None, None, None...   \n",
       "2  {'hi_res': ['https://m.media-amazon.com/images...   \n",
       "3  {'hi_res': ['https://m.media-amazon.com/images...   \n",
       "4  {'hi_res': ['https://m.media-amazon.com/images...   \n",
       "\n",
       "                                              videos            store  \\\n",
       "0            {'title': [], 'url': [], 'user_id': []}            iteck   \n",
       "1            {'title': [], 'url': [], 'user_id': []}            Swivi   \n",
       "2  {'title': ['Wasabi Power Fujifilm NP-95 2 Pack...       BM Premium   \n",
       "3  {'title': ['ZAGG invisibleSHIELD Screen Protec...  InvisibleShield   \n",
       "4            {'title': [], 'url': [], 'user_id': []}      Mobile Edge   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [Electronics, Computers & Accessories, Tablet ...   \n",
       "1  [Electronics, Camera & Photo, Accessories, Vie...   \n",
       "2  [Electronics, Camera & Photo, Accessories, Bat...   \n",
       "3  [Electronics, Computers & Accessories, Tablet ...   \n",
       "4  [Electronics, Computers & Accessories, Laptop ...   \n",
       "\n",
       "                                             details        asin  \\\n",
       "0  {\"Brand Name\": \"iTeck\", \"Item Weight\": \"0.8 ou...  B00CFGZAT8   \n",
       "1  {\"Product Dimensions\": \"3.8 x 4.3 x 6.9 inches...  B00BGFTSGU   \n",
       "2  {\"Package Dimensions\": \"6.46 x 3.94 x 1.85 inc...  B00AZOIG9I   \n",
       "3  {\"Standing screen display size\": \"9.7 Inches\",...  B00F361KLO   \n",
       "4  {\"Product Dimensions\": \"4.25 x 18 x 13.5 inche...  B0029L7N76   \n",
       "\n",
       "  bought_together subtitle author  \\\n",
       "0            None     None   None   \n",
       "1            None     None   None   \n",
       "2            None     None   None   \n",
       "3            None     None   None   \n",
       "4            None     None   None   \n",
       "\n",
       "                                             reviews  \\\n",
       "0  [{'rating': 1.0, 'title': 'One Star', 'text': ...   \n",
       "1  [{'rating': 4.0, 'title': 'Great except for on...   \n",
       "2  [{'rating': 5.0, 'title': 'Five Stars', 'text'...   \n",
       "3  [{'rating': 5.0, 'title': 'Five Stars', 'text'...   \n",
       "4  [{'rating': 2.0, 'title': 'Too big!', 'text': ...   \n",
       "\n",
       "                                             q_and_a  \n",
       "0  [{'questionType': 'yes/no', 'asin': 'B00CFGZAT...  \n",
       "1  [{'questionType': 'yes/no', 'asin': 'B00BGFTSG...  \n",
       "2  [{'questionType': 'open-ended', 'asin': 'B00AZ...  \n",
       "3  [{'questionType': 'yes/no', 'asin': 'B00F361KL...  \n",
       "4  [{'questionType': 'yes/no', 'asin': 'B0029L7N7...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>asin</th>\n",
       "      <th>bought_together</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>reviews</th>\n",
       "      <th>q_and_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Portable Audio &amp; Accessories</td>\n",
       "      <td>Dock Audio Extender Adapter Converter Cable fo...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>45</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>{'hi_res': [None, None], 'large': ['https://m....</td>\n",
       "      <td>{'title': [], 'url': [], 'user_id': []}</td>\n",
       "      <td>iteck</td>\n",
       "      <td>[Electronics, Computers &amp; Accessories, Tablet ...</td>\n",
       "      <td>{\"Brand Name\": \"iTeck\", \"Item Weight\": \"0.8 ou...</td>\n",
       "      <td>B00CFGZAT8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rating': 1.0, 'title': 'One Star', 'text': ...</td>\n",
       "      <td>[{'questionType': 'yes/no', 'asin': 'B00CFGZAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>GGS Swivi HD DSLR LCD Universal Foldable Viewf...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>52</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>149.0</td>\n",
       "      <td>{'hi_res': [None, None, None, None, None, None...</td>\n",
       "      <td>{'title': [], 'url': [], 'user_id': []}</td>\n",
       "      <td>Swivi</td>\n",
       "      <td>[Electronics, Camera &amp; Photo, Accessories, Vie...</td>\n",
       "      <td>{\"Product Dimensions\": \"3.8 x 4.3 x 6.9 inches...</td>\n",
       "      <td>B00BGFTSGU</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rating': 4.0, 'title': 'Great except for on...</td>\n",
       "      <td>[{'questionType': 'yes/no', 'asin': 'B00BGFTSG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>BM Premium 2-Pack of NP-85 Batteries and Charg...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>345</td>\n",
       "      <td>[NP85 Li-ion Battery for FujiFilm FinePix SL24...</td>\n",
       "      <td>[Bring your digital camera back to life with a...</td>\n",
       "      <td>22.99</td>\n",
       "      <td>{'hi_res': ['https://m.media-amazon.com/images...</td>\n",
       "      <td>{'title': ['Wasabi Power Fujifilm NP-95 2 Pack...</td>\n",
       "      <td>BM Premium</td>\n",
       "      <td>[Electronics, Camera &amp; Photo, Accessories, Bat...</td>\n",
       "      <td>{\"Package Dimensions\": \"6.46 x 3.94 x 1.85 inc...</td>\n",
       "      <td>B00AZOIG9I</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rating': 5.0, 'title': 'Five Stars', 'text'...</td>\n",
       "      <td>[{'questionType': 'open-ended', 'asin': 'B00AZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computers</td>\n",
       "      <td>ZAGG InvisibleShield HD – EZ Apply Film Screen...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>158</td>\n",
       "      <td>[100% Clear: Independent light transmission te...</td>\n",
       "      <td>[Exceptionally clear, unbelievably thin, and v...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'hi_res': ['https://m.media-amazon.com/images...</td>\n",
       "      <td>{'title': ['ZAGG invisibleSHIELD Screen Protec...</td>\n",
       "      <td>InvisibleShield</td>\n",
       "      <td>[Electronics, Computers &amp; Accessories, Tablet ...</td>\n",
       "      <td>{\"Standing screen display size\": \"9.7 Inches\",...</td>\n",
       "      <td>B00F361KLO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rating': 5.0, 'title': 'Five Stars', 'text'...</td>\n",
       "      <td>[{'questionType': 'yes/no', 'asin': 'B00F361KL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computers</td>\n",
       "      <td>Mobile Edge ECO Laptop Messenger (Eco-Friendly...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>[Fits 17.3-Inch laptops. Laptop compartment di...</td>\n",
       "      <td>[The 17.3-Inch ECO Messenger is the latest add...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'hi_res': ['https://m.media-amazon.com/images...</td>\n",
       "      <td>{'title': [], 'url': [], 'user_id': []}</td>\n",
       "      <td>Mobile Edge</td>\n",
       "      <td>[Electronics, Computers &amp; Accessories, Laptop ...</td>\n",
       "      <td>{\"Product Dimensions\": \"4.25 x 18 x 13.5 inche...</td>\n",
       "      <td>B0029L7N76</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rating': 2.0, 'title': 'Too big!', 'text': ...</td>\n",
       "      <td>[{'questionType': 'yes/no', 'asin': 'B0029L7N7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:15:08.051308Z",
     "start_time": "2025-06-29T19:14:53.421358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the final output path for the master dataset\n",
    "master_output_path = os.path.join(data_dir, \"master_chatbot_dataset.parquet\")\n",
    "\n",
    "print(f\"\\n--- Saving Final Master Dataset ---\")\n",
    "print(f\"Saving to: {master_output_path}\")\n",
    "\n",
    "try:\n",
    "    # Save the final DataFrame to a single Parquet file\n",
    "    master_df.to_parquet(master_output_path, index=False)\n",
    "    print(\"\\n✅ Project Complete! ✅\")\n",
    "    print(\"Your master dataset is now saved and ready for building your chatbot.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during the final save: {e}\")"
   ],
   "id": "72ce404b77a00918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Final Master Dataset ---\n",
      "Saving to: ../data/gold_standard_data/master_chatbot_dataset.parquet\n",
      "\n",
      "✅ Project Complete! ✅\n",
      "Your master dataset is now saved and ready for building your chatbot.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb52c8f346c0824d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa2afefabd483a77"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
